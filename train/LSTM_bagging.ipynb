{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0fd614-06e9-4bf3-8a84-2f5c0e0cf5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_val = pd.read_csv('X_val.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_val = pd.read_csv('y_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e959fbe-6030-48a8-9316-132aac6e6b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Traffic_Volume</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>week_number_sin</th>\n",
       "      <th>week_number_cos</th>\n",
       "      <th>quarter_sin</th>\n",
       "      <th>quarter_cos</th>\n",
       "      <th>four_month_sin</th>\n",
       "      <th>four_month_cos</th>\n",
       "      <th>half_year_sin</th>\n",
       "      <th>half_year_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.277074</td>\n",
       "      <td>-0.101973</td>\n",
       "      <td>-0.381232</td>\n",
       "      <td>-0.134296</td>\n",
       "      <td>1.188318</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.603098</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>-1.822248</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>6.432491e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.499842</td>\n",
       "      <td>-1.172810</td>\n",
       "      <td>-1.231675</td>\n",
       "      <td>-1.351007</td>\n",
       "      <td>-0.306376</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-2.009760</td>\n",
       "      <td>0.485982</td>\n",
       "      <td>0.014492</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>6.432491e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.332017</td>\n",
       "      <td>-1.617308</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.712218</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.018358</td>\n",
       "      <td>-0.774021</td>\n",
       "      <td>1.760901</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.266043</td>\n",
       "      <td>-1.475877</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.189412</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.255540</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>0.526370</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.734597</td>\n",
       "      <td>-1.081889</td>\n",
       "      <td>-1.750238</td>\n",
       "      <td>-1.246446</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.579803</td>\n",
       "      <td>3.157190</td>\n",
       "      <td>-0.994210</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.596741</td>\n",
       "      <td>-2.152726</td>\n",
       "      <td>-1.978405</td>\n",
       "      <td>-2.396618</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.893057</td>\n",
       "      <td>0.670783</td>\n",
       "      <td>0.511315</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.729602</td>\n",
       "      <td>-2.425486</td>\n",
       "      <td>-2.382884</td>\n",
       "      <td>-2.339584</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.684221</td>\n",
       "      <td>-0.354020</td>\n",
       "      <td>1.685625</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.084275</td>\n",
       "      <td>-1.890068</td>\n",
       "      <td>-1.750238</td>\n",
       "      <td>-1.293973</td>\n",
       "      <td>0.239762</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.464375</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>1.309243</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.569726</td>\n",
       "      <td>-0.980867</td>\n",
       "      <td>-1.573926</td>\n",
       "      <td>-1.132379</td>\n",
       "      <td>1.087714</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.913940</td>\n",
       "      <td>1.157984</td>\n",
       "      <td>1.203857</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.152162</td>\n",
       "      <td>-1.910272</td>\n",
       "      <td>-1.812465</td>\n",
       "      <td>-1.864307</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.934824</td>\n",
       "      <td>0.368382</td>\n",
       "      <td>2.619050</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050584</td>\n",
       "      <td>-1.970886</td>\n",
       "      <td>-1.978405</td>\n",
       "      <td>-1.607657</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.203331</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>2.363111</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.054306</td>\n",
       "      <td>-0.940458</td>\n",
       "      <td>-0.723483</td>\n",
       "      <td>-0.818696</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.245666</td>\n",
       "      <td>0.435582</td>\n",
       "      <td>0.390873</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.257369</td>\n",
       "      <td>-0.960663</td>\n",
       "      <td>-0.941280</td>\n",
       "      <td>-1.027818</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.039242</td>\n",
       "      <td>-0.774021</td>\n",
       "      <td>-0.572663</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.318988</td>\n",
       "      <td>-1.193014</td>\n",
       "      <td>-0.941280</td>\n",
       "      <td>-1.503095</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.498112</td>\n",
       "      <td>-0.354020</td>\n",
       "      <td>0.420984</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.077282</td>\n",
       "      <td>-1.435468</td>\n",
       "      <td>-1.459842</td>\n",
       "      <td>-1.607657</td>\n",
       "      <td>-0.191399</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.456345</td>\n",
       "      <td>1.157984</td>\n",
       "      <td>1.143636</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.110522</td>\n",
       "      <td>-1.718330</td>\n",
       "      <td>-1.688010</td>\n",
       "      <td>-1.398534</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.936667</td>\n",
       "      <td>-0.118819</td>\n",
       "      <td>-0.422110</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.199429</td>\n",
       "      <td>-1.587001</td>\n",
       "      <td>-1.231675</td>\n",
       "      <td>-1.864307</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.018358</td>\n",
       "      <td>0.973184</td>\n",
       "      <td>-0.994210</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.108524</td>\n",
       "      <td>-1.344548</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.027818</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.015379</td>\n",
       "      <td>0.301182</td>\n",
       "      <td>-0.045729</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.267358</td>\n",
       "      <td>-0.839436</td>\n",
       "      <td>-1.407986</td>\n",
       "      <td>-0.980290</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.861731</td>\n",
       "      <td>1.645186</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.979342</td>\n",
       "      <td>-1.940579</td>\n",
       "      <td>-1.812465</td>\n",
       "      <td>-2.073429</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.655307</td>\n",
       "      <td>0.250782</td>\n",
       "      <td>2.287835</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.269040</td>\n",
       "      <td>-1.869864</td>\n",
       "      <td>-1.688010</td>\n",
       "      <td>-1.968868</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.195300</td>\n",
       "      <td>-0.421220</td>\n",
       "      <td>2.634105</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.226401</td>\n",
       "      <td>-1.930477</td>\n",
       "      <td>-1.926549</td>\n",
       "      <td>-1.607657</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.130806</td>\n",
       "      <td>0.301182</td>\n",
       "      <td>1.550127</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.020342</td>\n",
       "      <td>-1.485979</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.560129</td>\n",
       "      <td>-0.306376</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.882615</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>-0.331779</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.229398</td>\n",
       "      <td>-1.496081</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.351007</td>\n",
       "      <td>-0.335120</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.183015</td>\n",
       "      <td>1.090784</td>\n",
       "      <td>-1.129707</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105527</td>\n",
       "      <td>-1.900170</td>\n",
       "      <td>-2.092489</td>\n",
       "      <td>-2.025901</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.289845</td>\n",
       "      <td>0.183582</td>\n",
       "      <td>1.038249</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.334289</td>\n",
       "      <td>-2.486100</td>\n",
       "      <td>-2.559196</td>\n",
       "      <td>-2.235023</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.039242</td>\n",
       "      <td>-1.076422</td>\n",
       "      <td>2.302890</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.581714</td>\n",
       "      <td>-1.940579</td>\n",
       "      <td>-1.978405</td>\n",
       "      <td>-1.759745</td>\n",
       "      <td>-0.335120</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.224783</td>\n",
       "      <td>0.183582</td>\n",
       "      <td>1.203857</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.217411</td>\n",
       "      <td>-1.566797</td>\n",
       "      <td>-2.154717</td>\n",
       "      <td>-1.759745</td>\n",
       "      <td>-0.306376</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.540448</td>\n",
       "      <td>-0.051619</td>\n",
       "      <td>1.068359</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.739592</td>\n",
       "      <td>-2.294157</td>\n",
       "      <td>-2.382884</td>\n",
       "      <td>-1.864307</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.381409</td>\n",
       "      <td>-0.841222</td>\n",
       "      <td>1.053304</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.096537</td>\n",
       "      <td>-1.597103</td>\n",
       "      <td>-1.573926</td>\n",
       "      <td>-1.351007</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.966149</td>\n",
       "      <td>-0.606021</td>\n",
       "      <td>-0.346834</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.670938</td>\n",
       "      <td>-1.536490</td>\n",
       "      <td>-1.522070</td>\n",
       "      <td>-1.293973</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.017790</td>\n",
       "      <td>-0.841222</td>\n",
       "      <td>0.646812</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-1.017302</td>\n",
       "      <td>-0.738414</td>\n",
       "      <td>-0.827196</td>\n",
       "      <td>-0.238857</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.339074</td>\n",
       "      <td>0.435582</td>\n",
       "      <td>0.165044</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.908416</td>\n",
       "      <td>-0.526267</td>\n",
       "      <td>-1.055363</td>\n",
       "      <td>-0.609573</td>\n",
       "      <td>3.890265</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.717390</td>\n",
       "      <td>0.183582</td>\n",
       "      <td>0.134934</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.173456</td>\n",
       "      <td>-1.688023</td>\n",
       "      <td>-1.459842</td>\n",
       "      <td>-1.921340</td>\n",
       "      <td>1.375155</td>\n",
       "      <td>2.212056</td>\n",
       "      <td>-1.821808</td>\n",
       "      <td>2.485188</td>\n",
       "      <td>1.068359</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.753577</td>\n",
       "      <td>-2.001193</td>\n",
       "      <td>-1.812465</td>\n",
       "      <td>-2.235023</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>13.960551</td>\n",
       "      <td>1.456913</td>\n",
       "      <td>-0.118819</td>\n",
       "      <td>1.580238</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.614722</td>\n",
       "      <td>-2.253748</td>\n",
       "      <td>-2.445112</td>\n",
       "      <td>-2.292057</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>11.610852</td>\n",
       "      <td>0.621570</td>\n",
       "      <td>-0.354020</td>\n",
       "      <td>2.317945</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.820507</td>\n",
       "      <td>-1.859761</td>\n",
       "      <td>-1.926549</td>\n",
       "      <td>-1.398534</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>11.610852</td>\n",
       "      <td>-0.182447</td>\n",
       "      <td>-0.538821</td>\n",
       "      <td>1.429685</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-1.019300</td>\n",
       "      <td>-1.698126</td>\n",
       "      <td>-1.864321</td>\n",
       "      <td>-2.025901</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>10.044386</td>\n",
       "      <td>0.830406</td>\n",
       "      <td>-0.354020</td>\n",
       "      <td>1.038249</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.395950</td>\n",
       "      <td>-1.738535</td>\n",
       "      <td>-2.092489</td>\n",
       "      <td>-0.980290</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>7.694687</td>\n",
       "      <td>-0.245098</td>\n",
       "      <td>-0.051619</td>\n",
       "      <td>0.285486</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.647688</td>\n",
       "      <td>-0.748516</td>\n",
       "      <td>-0.547172</td>\n",
       "      <td>-0.980290</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>6.128221</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>1.040384</td>\n",
       "      <td>-0.933989</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.091268</td>\n",
       "      <td>-1.102094</td>\n",
       "      <td>-1.003507</td>\n",
       "      <td>-1.455568</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>2.212056</td>\n",
       "      <td>0.569361</td>\n",
       "      <td>0.250782</td>\n",
       "      <td>-0.301668</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.376970</td>\n",
       "      <td>-1.001072</td>\n",
       "      <td>-0.827196</td>\n",
       "      <td>-0.980290</td>\n",
       "      <td>0.024182</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.256108</td>\n",
       "      <td>0.855584</td>\n",
       "      <td>-1.280259</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.875450</td>\n",
       "      <td>-1.455672</td>\n",
       "      <td>-1.573926</td>\n",
       "      <td>-1.607657</td>\n",
       "      <td>-0.335120</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.174985</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>1.128580</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.685648</td>\n",
       "      <td>-2.041602</td>\n",
       "      <td>-2.040633</td>\n",
       "      <td>-2.235023</td>\n",
       "      <td>-0.306376</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.174985</td>\n",
       "      <td>-0.908422</td>\n",
       "      <td>1.489906</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.121236</td>\n",
       "      <td>-2.102215</td>\n",
       "      <td>-1.978405</td>\n",
       "      <td>-1.816779</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.955708</td>\n",
       "      <td>-1.630824</td>\n",
       "      <td>1.655514</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.485107e-01</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.298052</td>\n",
       "      <td>-1.415263</td>\n",
       "      <td>-1.522070</td>\n",
       "      <td>-0.980290</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.913372</td>\n",
       "      <td>-0.169220</td>\n",
       "      <td>1.745846</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.485107e-01</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.214414</td>\n",
       "      <td>-0.263609</td>\n",
       "      <td>-0.142693</td>\n",
       "      <td>-0.181823</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.130238</td>\n",
       "      <td>1.947587</td>\n",
       "      <td>0.134934</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.485107e-01</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-1.132182</td>\n",
       "      <td>-0.233302</td>\n",
       "      <td>-1.055363</td>\n",
       "      <td>-0.238857</td>\n",
       "      <td>5.614912</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.966149</td>\n",
       "      <td>1.393185</td>\n",
       "      <td>-1.174873</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.485107e-01</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.420924</td>\n",
       "      <td>-1.667819</td>\n",
       "      <td>-1.750238</td>\n",
       "      <td>-1.664690</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.579803</td>\n",
       "      <td>0.973184</td>\n",
       "      <td>0.827475</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.485107e-01</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.868457</td>\n",
       "      <td>-1.597103</td>\n",
       "      <td>-1.459842</td>\n",
       "      <td>-1.607657</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.091451</td>\n",
       "      <td>0.973184</td>\n",
       "      <td>1.580238</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.485107e-01</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Traffic_Volume      tavg      tmin      tmax      prcp       snow  \\\n",
       "0        -0.277074 -0.101973 -0.381232 -0.134296  1.188318  -0.137643   \n",
       "1        -0.499842 -1.172810 -1.231675 -1.351007 -0.306376  -0.137643   \n",
       "2        -0.332017 -1.617308 -1.345759 -1.712218 -0.378236  -0.137643   \n",
       "3        -1.266043 -1.475877 -1.345759 -1.189412 -0.378236  -0.137643   \n",
       "4        -0.734597 -1.081889 -1.750238 -1.246446 -0.378236  -0.137643   \n",
       "5        -0.596741 -2.152726 -1.978405 -2.396618 -0.378236  -0.137643   \n",
       "6        -0.729602 -2.425486 -2.382884 -2.339584 -0.378236  -0.137643   \n",
       "7        -0.084275 -1.890068 -1.750238 -1.293973  0.239762  -0.137643   \n",
       "8        -1.569726 -0.980867 -1.573926 -1.132379  1.087714  -0.137643   \n",
       "9        -1.152162 -1.910272 -1.812465 -1.864307 -0.378236  -0.137643   \n",
       "10        0.050584 -1.970886 -1.978405 -1.607657 -0.378236  -0.137643   \n",
       "11       -0.054306 -0.940458 -0.723483 -0.818696 -0.378236  -0.137643   \n",
       "12        0.257369 -0.960663 -0.941280 -1.027818 -0.378236  -0.137643   \n",
       "13       -1.318988 -1.193014 -0.941280 -1.503095 -0.378236  -0.137643   \n",
       "14       -0.077282 -1.435468 -1.459842 -1.607657 -0.191399  -0.137643   \n",
       "15        0.110522 -1.718330 -1.688010 -1.398534 -0.378236  -0.137643   \n",
       "16        0.199429 -1.587001 -1.231675 -1.864307 -0.378236  -0.137643   \n",
       "17        0.108524 -1.344548 -1.345759 -1.027818 -0.378236  -0.137643   \n",
       "18        0.267358 -0.839436 -1.407986 -0.980290 -0.378236  -0.137643   \n",
       "19       -0.979342 -1.940579 -1.812465 -2.073429 -0.378236  -0.137643   \n",
       "20       -1.269040 -1.869864 -1.688010 -1.968868 -0.378236  -0.137643   \n",
       "21        0.226401 -1.930477 -1.926549 -1.607657 -0.378236  -0.137643   \n",
       "22       -0.020342 -1.485979 -1.345759 -1.560129 -0.306376  -0.137643   \n",
       "23        0.229398 -1.496081 -1.345759 -1.351007 -0.335120  -0.137643   \n",
       "24        0.105527 -1.900170 -2.092489 -2.025901 -0.378236  -0.137643   \n",
       "25        0.334289 -2.486100 -2.559196 -2.235023 -0.378236  -0.137643   \n",
       "26       -1.581714 -1.940579 -1.978405 -1.759745 -0.335120  -0.137643   \n",
       "27        0.217411 -1.566797 -2.154717 -1.759745 -0.306376  -0.137643   \n",
       "28       -0.739592 -2.294157 -2.382884 -1.864307 -0.378236  -0.137643   \n",
       "29        0.096537 -1.597103 -1.573926 -1.351007 -0.378236  -0.137643   \n",
       "30        0.670938 -1.536490 -1.522070 -1.293973 -0.378236  -0.137643   \n",
       "31       -1.017302 -0.738414 -0.827196 -0.238857 -0.378236  -0.137643   \n",
       "32       -0.908416 -0.526267 -1.055363 -0.609573  3.890265  -0.137643   \n",
       "33        0.173456 -1.688023 -1.459842 -1.921340  1.375155   2.212056   \n",
       "34       -0.753577 -2.001193 -1.812465 -2.235023 -0.378236  13.960551   \n",
       "35       -0.614722 -2.253748 -2.445112 -2.292057 -0.378236  11.610852   \n",
       "36       -0.820507 -1.859761 -1.926549 -1.398534 -0.378236  11.610852   \n",
       "37       -1.019300 -1.698126 -1.864321 -2.025901 -0.378236  10.044386   \n",
       "38       -0.395950 -1.738535 -2.092489 -0.980290 -0.378236   7.694687   \n",
       "39       -0.647688 -0.748516 -0.547172 -0.980290 -0.378236   6.128221   \n",
       "40       -0.091268 -1.102094 -1.003507 -1.455568 -0.378236   2.212056   \n",
       "41       -0.376970 -1.001072 -0.827196 -0.980290  0.024182  -0.137643   \n",
       "42       -0.875450 -1.455672 -1.573926 -1.607657 -0.335120  -0.137643   \n",
       "43       -0.685648 -2.041602 -2.040633 -2.235023 -0.306376  -0.137643   \n",
       "44       -0.121236 -2.102215 -1.978405 -1.816779 -0.378236  -0.137643   \n",
       "45       -0.298052 -1.415263 -1.522070 -0.980290 -0.378236  -0.137643   \n",
       "46        0.214414 -0.263609 -0.142693 -0.181823 -0.378236  -0.137643   \n",
       "47       -1.132182 -0.233302 -1.055363 -0.238857  5.614912  -0.137643   \n",
       "48       -0.420924 -1.667819 -1.750238 -1.664690 -0.378236  -0.137643   \n",
       "49       -0.868457 -1.597103 -1.459842 -1.607657 -0.378236  -0.137643   \n",
       "\n",
       "        wdir      wspd      pres  year  ...  month_sin  month_cos  \\\n",
       "0   1.603098  0.133181 -1.822248  2022  ...   0.500000   0.866025   \n",
       "1  -2.009760  0.485982  0.014492  2022  ...   0.500000   0.866025   \n",
       "2   1.018358 -0.774021  1.760901  2022  ...   0.500000   0.866025   \n",
       "3  -0.255540  0.737983  0.526370  2022  ...   0.500000   0.866025   \n",
       "4   0.579803  3.157190 -0.994210  2022  ...   0.500000   0.866025   \n",
       "5   0.893057  0.670783  0.511315  2022  ...   0.500000   0.866025   \n",
       "6   0.684221 -0.354020  1.685625  2022  ...   0.500000   0.866025   \n",
       "7  -0.464375  0.737983  1.309243  2022  ...   0.500000   0.866025   \n",
       "8   0.913940  1.157984  1.203857  2022  ...   0.500000   0.866025   \n",
       "9   0.934824  0.368382  2.619050  2022  ...   0.500000   0.866025   \n",
       "10 -0.203331  0.133181  2.363111  2022  ...   0.500000   0.866025   \n",
       "11  0.245666  0.435582  0.390873  2022  ...   0.500000   0.866025   \n",
       "12  1.039242 -0.774021 -0.572663  2022  ...   0.500000   0.866025   \n",
       "13 -1.498112 -0.354020  0.420984  2022  ...   0.500000   0.866025   \n",
       "14 -1.456345  1.157984  1.143636  2022  ...   0.500000   0.866025   \n",
       "15 -1.936667 -0.118819 -0.422110  2022  ...   0.500000   0.866025   \n",
       "16  1.018358  0.973184 -0.994210  2022  ...   0.500000   0.866025   \n",
       "17 -0.015379  0.301182 -0.045729  2022  ...   0.500000   0.866025   \n",
       "18  0.861731  1.645186 -0.000563  2022  ...   0.500000   0.866025   \n",
       "19  1.655307  0.250782  2.287835  2022  ...   0.500000   0.866025   \n",
       "20 -1.195300 -0.421220  2.634105  2022  ...   0.500000   0.866025   \n",
       "21  0.130806  0.301182  1.550127  2022  ...   0.500000   0.866025   \n",
       "22  0.882615  0.737983 -0.331779  2022  ...   0.500000   0.866025   \n",
       "23  0.183015  1.090784 -1.129707  2022  ...   0.500000   0.866025   \n",
       "24  1.289845  0.183582  1.038249  2022  ...   0.500000   0.866025   \n",
       "25  1.039242 -1.076422  2.302890  2022  ...   0.500000   0.866025   \n",
       "26  0.224783  0.183582  1.203857  2022  ...   0.500000   0.866025   \n",
       "27  1.540448 -0.051619  1.068359  2022  ...   0.500000   0.866025   \n",
       "28  0.381409 -0.841222  1.053304  2022  ...   0.500000   0.866025   \n",
       "29  0.966149 -0.606021 -0.346834  2022  ...   0.500000   0.866025   \n",
       "30 -1.017790 -0.841222  0.646812  2022  ...   0.500000   0.866025   \n",
       "31 -0.339074  0.435582  0.165044  2022  ...   0.866025   0.500000   \n",
       "32 -1.717390  0.183582  0.134934  2022  ...   0.866025   0.500000   \n",
       "33 -1.821808  2.485188  1.068359  2022  ...   0.866025   0.500000   \n",
       "34  1.456913 -0.118819  1.580238  2022  ...   0.866025   0.500000   \n",
       "35  0.621570 -0.354020  2.317945  2022  ...   0.866025   0.500000   \n",
       "36 -0.182447 -0.538821  1.429685  2022  ...   0.866025   0.500000   \n",
       "37  0.830406 -0.354020  1.038249  2022  ...   0.866025   0.500000   \n",
       "38 -0.245098 -0.051619  0.285486  2022  ...   0.866025   0.500000   \n",
       "39  0.370968  1.040384 -0.933989  2022  ...   0.866025   0.500000   \n",
       "40  0.569361  0.250782 -0.301668  2022  ...   0.866025   0.500000   \n",
       "41  0.256108  0.855584 -1.280259  2022  ...   0.866025   0.500000   \n",
       "42  1.174985  0.737983  1.128580  2022  ...   0.866025   0.500000   \n",
       "43  1.174985 -0.908422  1.489906  2022  ...   0.866025   0.500000   \n",
       "44  0.955708 -1.630824  1.655514  2022  ...   0.866025   0.500000   \n",
       "45 -0.913372 -0.169220  1.745846  2022  ...   0.866025   0.500000   \n",
       "46 -0.130238  1.947587  0.134934  2022  ...   0.866025   0.500000   \n",
       "47  0.966149  1.393185 -1.174873  2022  ...   0.866025   0.500000   \n",
       "48  0.579803  0.973184  0.827475  2022  ...   0.866025   0.500000   \n",
       "49  1.091451  0.973184  1.580238  2022  ...   0.866025   0.500000   \n",
       "\n",
       "    week_number_sin  week_number_cos  quarter_sin   quarter_cos  \\\n",
       "0      6.432491e-16         1.000000          1.0  6.123234e-17   \n",
       "1      6.432491e-16         1.000000          1.0  6.123234e-17   \n",
       "2      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "3      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "4      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "5      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "6      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "7      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "8      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "9      2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "10     2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "11     2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "12     2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "13     2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "14     2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "15     2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "16     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "17     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "18     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "19     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "20     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "21     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "22     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "23     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "24     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "25     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "26     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "27     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "28     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "29     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "30     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "31     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "32     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "33     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "34     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "35     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "36     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "37     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "38     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "39     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "40     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "41     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "42     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "43     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "44     7.485107e-01         0.663123          1.0  6.123234e-17   \n",
       "45     7.485107e-01         0.663123          1.0  6.123234e-17   \n",
       "46     7.485107e-01         0.663123          1.0  6.123234e-17   \n",
       "47     7.485107e-01         0.663123          1.0  6.123234e-17   \n",
       "48     7.485107e-01         0.663123          1.0  6.123234e-17   \n",
       "49     7.485107e-01         0.663123          1.0  6.123234e-17   \n",
       "\n",
       "    four_month_sin  four_month_cos  half_year_sin  half_year_cos  \n",
       "0         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "1         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "2         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "3         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "4         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "5         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "6         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "7         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "8         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "9         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "10        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "11        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "12        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "13        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "14        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "15        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "16        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "17        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "18        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "19        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "20        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "21        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "22        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "23        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "24        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "25        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "26        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "27        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "28        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "29        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "30        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "31        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "32        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "33        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "34        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "35        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "36        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "37        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "38        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "39        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "40        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "41        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "42        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "43        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "44        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "45        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "46        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "47        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "48        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "49        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "\n",
       "[50 rows x 35 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de9504f-8a3f-462a-bf96-5768dabef409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "X_train_numeric = X_train.select_dtypes(include=[float, int])\n",
    "X_val_numeric = X_val.select_dtypes(include=[float, int])\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_numeric.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)  \n",
    "X_val_tensor = torch.tensor(X_val_numeric.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7803604b-d213-425d-9918-660743c6415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_labels = y_train_tensor\n",
    "y_val_labels = y_val_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab796678-0292-4d0a-abf8-0d587b09a10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([705, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77093a83-c215-4862-9fd4-7a434bfa94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined = torch.cat((X_train_tensor, y_train_tensor), dim=1)\n",
    "val_combined = torch.cat((X_val_tensor, y_val_tensor), dim=1)\n",
    "full_dataset = torch.cat((train_combined, val_combined), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3ccb29f-a881-4518-965d-9632e6c456a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bc2357eb-e4ee-4735-be0a-777eaa6f66eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_sequences shape: torch.Size([878, 4, 35])\n",
      "y_sequences shape: torch.Size([878])\n"
     ]
    }
   ],
   "source": [
    "def create_subsequences(full_data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    X_data = full_data[:, :-1]  \n",
    "    y_data = full_data[:, -1]  \n",
    "\n",
    "    for i in range(seq_length, len(full_data)):\n",
    "        X_seq = X_data[i - seq_length:i]\n",
    "        y_seq = y_data[i] \n",
    "        sequences.append(X_seq)\n",
    "        targets.append(y_seq)\n",
    "        \n",
    "    return torch.stack(sequences), torch.tensor(targets).long()\n",
    "\n",
    "sequence_length = 4\n",
    "X_sequences, y_sequences = create_subsequences(full_dataset, sequence_length)\n",
    "\n",
    "print(f\"X_sequences shape: {X_sequences.shape}\")\n",
    "print(f\"y_sequences shape: {y_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "84f8ce1e-e779-4b9e-a451-cc3de66b24ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.277074</td>\n",
       "      <td>-0.101973</td>\n",
       "      <td>-0.381232</td>\n",
       "      <td>-0.134296</td>\n",
       "      <td>1.188318</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.603098</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>-1.822248</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>6.432490e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.499842</td>\n",
       "      <td>-1.172810</td>\n",
       "      <td>-1.231675</td>\n",
       "      <td>-1.351007</td>\n",
       "      <td>-0.306376</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-2.009760</td>\n",
       "      <td>0.485982</td>\n",
       "      <td>0.014492</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>6.432490e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.332017</td>\n",
       "      <td>-1.617308</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.712218</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.018358</td>\n",
       "      <td>-0.774021</td>\n",
       "      <td>1.760901</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.266043</td>\n",
       "      <td>-1.475877</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.189412</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.255540</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>0.526370</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.734597</td>\n",
       "      <td>-1.081890</td>\n",
       "      <td>-1.750238</td>\n",
       "      <td>-1.246446</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.579803</td>\n",
       "      <td>3.157190</td>\n",
       "      <td>-0.994210</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>1.530042</td>\n",
       "      <td>0.726410</td>\n",
       "      <td>0.946289</td>\n",
       "      <td>0.550104</td>\n",
       "      <td>-0.335120</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.611129</td>\n",
       "      <td>1.275585</td>\n",
       "      <td>-1.566309</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>1.462113</td>\n",
       "      <td>0.675899</td>\n",
       "      <td>0.655894</td>\n",
       "      <td>0.654665</td>\n",
       "      <td>-0.306376</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.861731</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>-0.361889</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>2.159386</td>\n",
       "      <td>0.473854</td>\n",
       "      <td>0.541810</td>\n",
       "      <td>0.502577</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.237636</td>\n",
       "      <td>0.183582</td>\n",
       "      <td>0.360763</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>2.273267</td>\n",
       "      <td>0.514263</td>\n",
       "      <td>0.313642</td>\n",
       "      <td>0.607138</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.967993</td>\n",
       "      <td>-1.513224</td>\n",
       "      <td>0.752199</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>0.703903</td>\n",
       "      <td>0.776921</td>\n",
       "      <td>0.718121</td>\n",
       "      <td>0.654665</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.777629</td>\n",
       "      <td>-0.236420</td>\n",
       "      <td>0.887696</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>882 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.277074 -0.101973 -0.381232 -0.134296  1.188318 -0.137643  1.603098   \n",
       "1   -0.499842 -1.172810 -1.231675 -1.351007 -0.306376 -0.137643 -2.009760   \n",
       "2   -0.332017 -1.617308 -1.345759 -1.712218 -0.378236 -0.137643  1.018358   \n",
       "3   -1.266043 -1.475877 -1.345759 -1.189412 -0.378236 -0.137643 -0.255540   \n",
       "4   -0.734597 -1.081890 -1.750238 -1.246446 -0.378236 -0.137643  0.579803   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "877  1.530042  0.726410  0.946289  0.550104 -0.335120 -0.137643  0.611129   \n",
       "878  1.462113  0.675899  0.655894  0.654665 -0.306376 -0.137643  0.861731   \n",
       "879  2.159386  0.473854  0.541810  0.502577 -0.378236 -0.137643  1.237636   \n",
       "880  2.273267  0.514263  0.313642  0.607138 -0.378236 -0.137643 -1.967993   \n",
       "881  0.703903  0.776921  0.718121  0.654665 -0.378236 -0.137643 -0.777629   \n",
       "\n",
       "           7         8       9   ...        26            27        28  \\\n",
       "0    0.133181 -1.822248  2022.0  ...  0.866025  6.432490e-16  1.000000   \n",
       "1    0.485982  0.014492  2022.0  ...  0.866025  6.432490e-16  1.000000   \n",
       "2   -0.774021  1.760901  2022.0  ...  0.866025  1.205367e-01  0.992709   \n",
       "3    0.737983  0.526370  2022.0  ...  0.866025  1.205367e-01  0.992709   \n",
       "4    3.157190 -0.994210  2022.0  ...  0.866025  1.205367e-01  0.992709   \n",
       "..        ...       ...     ...  ...       ...           ...       ...   \n",
       "877  1.275585 -1.566309  2024.0  ... -0.866025  4.647232e-01 -0.885456   \n",
       "878  0.133181 -0.361889  2024.0  ... -0.866025  4.647232e-01 -0.885456   \n",
       "879  0.183582  0.360763  2024.0  ... -0.866025  4.647232e-01 -0.885456   \n",
       "880 -1.513224  0.752199  2024.0  ... -0.866025  4.647232e-01 -0.885456   \n",
       "881 -0.236420  0.887696  2024.0  ... -0.866025  4.647232e-01 -0.885456   \n",
       "\n",
       "               29            30        31   32            33   34   35  \n",
       "0    1.000000e+00  6.123234e-17  0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "1    1.000000e+00  6.123234e-17  0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "2    1.000000e+00  6.123234e-17  0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "3    1.000000e+00  6.123234e-17  0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "4    1.000000e+00  6.123234e-17  0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "..            ...           ...       ...  ...           ...  ...  ...  \n",
       "877  1.224647e-16 -1.000000e+00 -0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "878  1.224647e-16 -1.000000e+00 -0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "879  1.224647e-16 -1.000000e+00 -0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "880  1.224647e-16 -1.000000e+00 -0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "881  1.224647e-16 -1.000000e+00 -0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "\n",
       "[882 rows x 36 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset_df = pd.DataFrame(full_dataset.numpy())\n",
    "full_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "4ac706b7-eebf-46ed-a1d6-773eaa59bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def create_subsequences_with_duplication(full_data, seq_length, random_duplicate):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    X_data = full_data[:, :-1]  \n",
    "    y_data = full_data[:, -1]  \n",
    "\n",
    "    for i in range(seq_length, len(full_data)):\n",
    "        X_seq = X_data[i - seq_length:i]\n",
    "        y_seq = y_data[i]\n",
    "        \n",
    "        sequences.append(X_seq)\n",
    "        targets.append(y_seq)\n",
    "\n",
    "        #had num_duplicates to see if duplicating data and bagging would help\n",
    "        if y_seq == 1:       \n",
    "            num_duplicates = 1\n",
    "            for _ in range(num_duplicates):\n",
    "                sequences.append(X_seq)\n",
    "                targets.append(y_seq)\n",
    "        elif y_seq == 2:\n",
    "            num_duplicates = 1\n",
    "            for _ in range(num_duplicates):\n",
    "                sequences.append(X_seq)\n",
    "                targets.append(y_seq)\n",
    "        elif y_seq == 3:\n",
    "            num_duplicates = 2\n",
    "            for _ in range(num_duplicates):\n",
    "                sequences.append(X_seq)\n",
    "                targets.append(y_seq)\n",
    "        elif y_seq == 4:\n",
    "            num_duplicates = 2\n",
    "            for _ in range(num_duplicates):\n",
    "                sequences.append(X_seq)\n",
    "                targets.append(y_seq)\n",
    "        \n",
    "        \n",
    "    return torch.stack(sequences), torch.tensor(targets).long()\n",
    "\n",
    "sequence_length = 3\n",
    "random_duplicate = 4\n",
    "\n",
    "X_sequences, y_sequences = create_subsequences_with_duplication(full_dataset, sequence_length, random_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "21adb1a8-b182-4d2d-8ba6-35e556d734cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([913])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "c489f938-e517-4931-b31e-a73e5fa96944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def shuffle_sequences(X_sequences, y_sequences):\n",
    "    combined = list(zip(X_sequences, y_sequences))\n",
    "    random.shuffle(combined)\n",
    "    X_shuffled, y_shuffled = zip(*combined)\n",
    "    \n",
    "    X_shuffled = torch.stack(X_shuffled)\n",
    "    y_shuffled = torch.tensor(y_shuffled).long()\n",
    "    \n",
    "    return X_shuffled, y_shuffled\n",
    "\n",
    "X_shuffled, y_shuffled = shuffle_sequences(X_sequences, y_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "dc49c7da-a77d-4920-bd1c-864c5d257d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sequences, y_sequences = X_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "d325fd7e-7f4f-432d-9f48-37143e584862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch shapes: X: torch.Size([16, 3, 35]), y: torch.Size([16])\n",
      "Validation batch shapes: X: torch.Size([16, 5, 35]), y: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(f\"Train batch shapes: X: {X_batch.shape}, y: {y_batch.shape}\")\n",
    "    break\n",
    "\n",
    "for X_batch, y_batch in val_loader:\n",
    "    print(f\"Validation batch shapes: X: {X_batch.shape}, y: {y_batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "a39114fa-f790-48b4-a27c-0637a215451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X_sequences.numpy()\n",
    "y_np = y_sequences.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "2b0e2125-fac1-4da0-b86d-210b0b3bfc4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00139011 0.02174472 0.19537303 0.39074607 0.39074607]\n"
     ]
    }
   ],
   "source": [
    "class_frequencies = np.array([0.9276, 0.0593, 0.0066, 0.0033, 0.0033])\n",
    "inverse_frequencies = 1 / class_frequencies\n",
    "alpha = inverse_frequencies / inverse_frequencies.sum()\n",
    "\n",
    "print(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "8e0302bc-90f8-4991-b1d8-fdc08b5dd68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of each value in y_np:\n",
      "Value 0: 92.77%\n",
      "Value 1: 5.91%\n",
      "Value 2: 0.66%\n",
      "Value 3: 0.33%\n",
      "Value 4: 0.33%\n",
      "\n",
      "Total: 100.00%\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_np, return_counts=True)\n",
    "\n",
    "percentages = counts / len(y_np) * 100\n",
    "\n",
    "value_percentages = dict(zip(unique, percentages))\n",
    "\n",
    "print(\"Percentage of each value in y_np:\")\n",
    "for value, percentage in value_percentages.items():\n",
    "    print(f\"Value {value}: {percentage:.2f}%\")\n",
    "\n",
    "print(f\"\\nTotal: {sum(percentages):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "b14babb6-b349-4925-ae6b-91e0002d2782",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_np = X_sequences.numpy()\n",
    "y_np = y_sequences.numpy()\n",
    "\n",
    "X_train_tensor = torch.tensor(X_np, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_np, dtype=torch.long)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train_tensor, y_train_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "b11aa87f-f0d0-4664-bcfc-57a8626b9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GatedLinearUnit(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(GatedLinearUnit, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.gate = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) * torch.sigmoid(self.gate(x))\n",
    "\n",
    "class LayerNormLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, bidirectional):\n",
    "        super(LayerNormLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers=num_layers,\n",
    "            batch_first=True, dropout=dropout, bidirectional=bidirectional\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * (2 if bidirectional else 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        return lstm_out\n",
    "\n",
    "class LayerDropLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, bidirectional, layer_drop_prob=0.2):\n",
    "        super(LayerDropLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.layer_drop_prob = layer_drop_prob\n",
    "        self.projection = nn.Linear(input_size, hidden_size * (2 if bidirectional else 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and torch.rand(1).item() < self.layer_drop_prob:\n",
    "            return self.projection(x)  #projecting input to match LSTM output size\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return lstm_out\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.long, device=x.device).unsqueeze(0)\n",
    "        pos_enc = self.pos_embedding(positions)\n",
    "        return x + pos_enc\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "\n",
    "class ResidualLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, bidirectional):\n",
    "        super(ResidualLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.projection = nn.Linear(hidden_size * (2 if bidirectional else 1), hidden_size * (2 if bidirectional else 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        projected_out = self.projection(lstm_out)\n",
    "        return projected_out\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, lstm_dropout=0.3, fcn_dropout=0.5, debug=False):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.bidirectional = True\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.num_heads = 8\n",
    "        self.hidden_size = hidden_size // (self.num_directions * 3)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Adjusted hidden_size: {self.hidden_size}\")\n",
    "        \n",
    "        self.embedding_size = input_size\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(self.embedding_size)\n",
    "\n",
    "        self.lstm1 = LayerNormLSTM(\n",
    "            self.embedding_size, self.hidden_size, num_layers=num_layers,\n",
    "            dropout=lstm_dropout, bidirectional=self.bidirectional\n",
    "        )\n",
    "        self.lstm2 = ResidualLSTM(\n",
    "            self.embedding_size, self.hidden_size, num_layers=num_layers,\n",
    "            dropout=lstm_dropout, bidirectional=self.bidirectional\n",
    "        )\n",
    "        self.lstm3 = LayerDropLSTM(\n",
    "            self.embedding_size, self.hidden_size, num_layers=num_layers,\n",
    "            dropout=lstm_dropout, bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "        self.combined_lstm_size = self.hidden_size * self.num_directions * 3\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Combined LSTM size: {self.combined_lstm_size}\")\n",
    "            print(f\"Number of heads: {self.num_heads}\")\n",
    "            print(f\"Is combined_lstm_size divisible by num_heads? {self.combined_lstm_size % self.num_heads == 0}\")\n",
    "\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.combined_lstm_size,\n",
    "            nhead=self.num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.transformer_layer, num_layers=3\n",
    "        )\n",
    "\n",
    "        self.attention = MultiHeadSelfAttention(self.combined_lstm_size, self.num_heads)\n",
    "        self.glu1 = GatedLinearUnit(self.combined_lstm_size, self.combined_lstm_size // 2)\n",
    "        self.glu2 = GatedLinearUnit(self.combined_lstm_size // 2, self.combined_lstm_size // 4)\n",
    "\n",
    "        self.fc = nn.Linear(self.combined_lstm_size // 4, num_classes)\n",
    "\n",
    "        self.layer_norm2 = nn.LayerNorm(self.combined_lstm_size // 2)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.combined_lstm_size // 4)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=fcn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after positional encoding: {x.shape}\")\n",
    "\n",
    "        lstm_out1 = self.lstm1(x)\n",
    "        lstm_out2 = self.lstm2(x)\n",
    "        lstm_out3 = self.lstm3(x)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape of lstm_out1: {lstm_out1.shape}\")\n",
    "            print(f\"Shape of lstm_out2: {lstm_out2.shape}\")\n",
    "            print(f\"Shape of lstm_out3: {lstm_out3.shape}\")\n",
    "\n",
    "        lstm_out_concat = torch.cat((lstm_out1, lstm_out2, lstm_out3), dim=-1)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after concatenation: {lstm_out_concat.shape}\")\n",
    "\n",
    "        transformer_out = self.transformer_encoder(lstm_out_concat)\n",
    "        transformer_out = lstm_out_concat + transformer_out  \n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after transformer: {transformer_out.shape}\")\n",
    "\n",
    "        attn_out = self.attention(transformer_out)\n",
    "        attn_out = transformer_out + attn_out  \n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after attention: {attn_out.shape}\")\n",
    "\n",
    "        global_avg_pool = torch.mean(attn_out, dim=1)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after global average pooling: {global_avg_pool.shape}\")\n",
    "\n",
    "        out = self.glu1(global_avg_pool)\n",
    "        out = self.layer_norm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after first GLU: {out.shape}\")\n",
    "\n",
    "        out = self.glu2(out)\n",
    "        out = self.layer_norm3(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after second GLU: {out.shape}\")\n",
    "\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Final output shape: {out.shape}\")\n",
    "\n",
    "        return out\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha, gamma=2.5, reduction='mean', label_smoothing=0.1):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = torch.tensor(alpha) \n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        num_classes = inputs.size(1)\n",
    "        smoothed_labels = F.one_hot(targets, num_classes=num_classes)\n",
    "        smoothed_labels = smoothed_labels * (1 - self.label_smoothing) + self.label_smoothing / num_classes\n",
    "\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=self.label_smoothing)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha[targets] * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "c1e816ac-b303-41e6-95ea-33721536a967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([913, 3, 35])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "54fd93b8-82c6-4801-9bd5-cf172c2a999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 35\n",
      "Initial hidden size: 384\n",
      "Adjusted hidden_size: 64\n",
      "Combined LSTM size: 384\n",
      "Number of heads: 8\n",
      "Is combined_lstm_size divisible by num_heads? True\n",
      "Shape after positional encoding: torch.Size([16, 3, 35])\n",
      "Shape of lstm_out1: torch.Size([16, 3, 128])\n",
      "Shape of lstm_out2: torch.Size([16, 3, 128])\n",
      "Shape of lstm_out3: torch.Size([16, 3, 128])\n",
      "Shape after concatenation: torch.Size([16, 3, 384])\n",
      "Shape after transformer: torch.Size([16, 3, 384])\n",
      "Shape after attention: torch.Size([16, 3, 384])\n",
      "Shape after global average pooling: torch.Size([16, 384])\n",
      "Shape after first GLU: torch.Size([16, 192])\n",
      "Shape after second GLU: torch.Size([16, 96])\n",
      "Final output shape: torch.Size([16, 5])\n"
     ]
    }
   ],
   "source": [
    "input_size = X_sequences.shape[2] #should match your X_sequences.shape[2] which is 35\n",
    "hidden_size = 384  #should be divisible by (num_directions * 3 * num_heads)\n",
    "num_layers = 4\n",
    "num_classes = 5\n",
    "\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Initial hidden size: {hidden_size}\")\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes, debug=True)\n",
    "\n",
    "batch_size = 16\n",
    "sequence_length = 3\n",
    "dummy_input = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "output = model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "3d53ca8f-3159-4171-8fc2-9ea8f7e4b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_sequences.shape[2]\n",
    "hidden_size = 384 #should be divisible by (num_directions * 3 * num_heads) \n",
    "num_layers = 2\n",
    "num_epochs = 55\n",
    "num_classes = 5\n",
    "bagging = 5\n",
    "batch_size = 16\n",
    "early_stop_patience = 10\n",
    "subset_size = 555\n",
    "alpha_np = [0.70, 0.27, 0.01, 0.01, 0.01]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "51ceec41-7c3e-4b93-8de0-b0359013407e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias_ih' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "                #setting forget gate bias to 1\n",
    "                n = param.size(0)\n",
    "                start, end = n // 4, n // 2\n",
    "                param.data[start:end].fill_(1.)\n",
    "            elif 'bias_hh' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.MultiheadAttention):\n",
    "        if m.in_proj_weight is not None:\n",
    "            nn.init.xavier_uniform_(m.in_proj_weight)\n",
    "        if m.out_proj.weight is not None:\n",
    "            nn.init.xavier_uniform_(m.out_proj.weight)\n",
    "        if m.in_proj_bias is not None:\n",
    "            nn.init.zeros_(m.in_proj_bias)\n",
    "        if m.out_proj.bias is not None:\n",
    "            nn.init.zeros_(m.out_proj.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "3f64a17d-03b0-48de-ae4f-3c60efaf42a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model 1...\n",
      "Class counts in the subset:\n",
      "Class 0: 517 samples (93.15%)\n",
      "Class 1: 33 samples (5.95%)\n",
      "Class 2: 3 samples (0.54%)\n",
      "Class 3: 1 samples (0.18%)\n",
      "Class 4: 1 samples (0.18%)\n",
      "Epoch [1/55], Train Loss: 0.0864, Train Acc: 84.68%, Best Acc: 84.68468%\n",
      "Epoch [2/55], Train Loss: 0.0420, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [3/55], Train Loss: 0.0380, Train Acc: 92.97%, Best Acc: 93.15315%\n",
      "Epoch [4/55], Train Loss: 0.0396, Train Acc: 92.79%, Best Acc: 93.15315%\n",
      "Epoch [5/55], Train Loss: 0.0369, Train Acc: 92.97%, Best Acc: 93.15315%\n",
      "Epoch [6/55], Train Loss: 0.0382, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [7/55], Train Loss: 0.0386, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [8/55], Train Loss: 0.0392, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [9/55], Train Loss: 0.0372, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [10/55], Train Loss: 0.0378, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [11/55], Train Loss: 0.0366, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [12/55], Train Loss: 0.0370, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [13/55], Train Loss: 0.0336, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [14/55], Train Loss: 0.0365, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [15/55], Train Loss: 0.0349, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [16/55], Train Loss: 0.0337, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [17/55], Train Loss: 0.0318, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [18/55], Train Loss: 0.0320, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [19/55], Train Loss: 0.0339, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [20/55], Train Loss: 0.0348, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [21/55], Train Loss: 0.0318, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [22/55], Train Loss: 0.0334, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [23/55], Train Loss: 0.0338, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [24/55], Train Loss: 0.0351, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [25/55], Train Loss: 0.0312, Train Acc: 92.97%, Best Acc: 93.15315%\n",
      "Epoch [26/55], Train Loss: 0.0335, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [27/55], Train Loss: 0.0316, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [28/55], Train Loss: 0.0324, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [29/55], Train Loss: 0.0334, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [30/55], Train Loss: 0.0341, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [31/55], Train Loss: 0.0308, Train Acc: 93.33%, Best Acc: 93.33333%\n",
      "Epoch [32/55], Train Loss: 0.0325, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [33/55], Train Loss: 0.0322, Train Acc: 92.79%, Best Acc: 93.33333%\n",
      "Epoch [34/55], Train Loss: 0.0300, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [35/55], Train Loss: 0.0346, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [36/55], Train Loss: 0.0305, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [37/55], Train Loss: 0.0293, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [38/55], Train Loss: 0.0314, Train Acc: 92.97%, Best Acc: 93.33333%\n",
      "Epoch [39/55], Train Loss: 0.0303, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [40/55], Train Loss: 0.0341, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [41/55], Train Loss: 0.0337, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [42/55], Train Loss: 0.0322, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [43/55], Train Loss: 0.0314, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [44/55], Train Loss: 0.0313, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [45/55], Train Loss: 0.0318, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [46/55], Train Loss: 0.0307, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [47/55], Train Loss: 0.0314, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [48/55], Train Loss: 0.0344, Train Acc: 92.97%, Best Acc: 93.33333%\n",
      "Epoch [49/55], Train Loss: 0.0340, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [50/55], Train Loss: 0.0306, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [51/55], Train Loss: 0.0339, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [52/55], Train Loss: 0.0311, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [53/55], Train Loss: 0.0313, Train Acc: 93.15%, Best Acc: 93.33333%\n",
      "Epoch [54/55], Train Loss: 0.0304, Train Acc: 92.97%, Best Acc: 93.33333%\n",
      "Epoch [55/55], Train Loss: 0.0302, Train Acc: 92.79%, Best Acc: 93.33333%\n",
      "Model 1 training complete with best accuracy: 93.33%\n",
      "\n",
      "Training model 2...\n",
      "Class counts in the subset:\n",
      "Class 0: 514 samples (92.61%)\n",
      "Class 1: 33 samples (5.95%)\n",
      "Class 2: 3 samples (0.54%)\n",
      "Class 3: 3 samples (0.54%)\n",
      "Class 4: 2 samples (0.36%)\n",
      "Epoch [1/55], Train Loss: 0.0785, Train Acc: 87.03%, Best Acc: 87.02703%\n",
      "Epoch [2/55], Train Loss: 0.0426, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [3/55], Train Loss: 0.0395, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [4/55], Train Loss: 0.0384, Train Acc: 92.25%, Best Acc: 92.61261%\n",
      "Epoch [5/55], Train Loss: 0.0394, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [6/55], Train Loss: 0.0390, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [7/55], Train Loss: 0.0379, Train Acc: 92.43%, Best Acc: 92.61261%\n",
      "Epoch [8/55], Train Loss: 0.0376, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [9/55], Train Loss: 0.0362, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [10/55], Train Loss: 0.0354, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [11/55], Train Loss: 0.0367, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [12/55], Train Loss: 0.0345, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [13/55], Train Loss: 0.0348, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [14/55], Train Loss: 0.0337, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [15/55], Train Loss: 0.0363, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [16/55], Train Loss: 0.0342, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [17/55], Train Loss: 0.0353, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [18/55], Train Loss: 0.0358, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [19/55], Train Loss: 0.0325, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [20/55], Train Loss: 0.0343, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [21/55], Train Loss: 0.0353, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [22/55], Train Loss: 0.0329, Train Acc: 92.07%, Best Acc: 92.61261%\n",
      "Epoch [23/55], Train Loss: 0.0364, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [24/55], Train Loss: 0.0311, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [25/55], Train Loss: 0.0335, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [26/55], Train Loss: 0.0330, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [27/55], Train Loss: 0.0345, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [28/55], Train Loss: 0.0341, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [29/55], Train Loss: 0.0332, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [30/55], Train Loss: 0.0321, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [31/55], Train Loss: 0.0326, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [32/55], Train Loss: 0.0346, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [33/55], Train Loss: 0.0323, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [34/55], Train Loss: 0.0330, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [35/55], Train Loss: 0.0331, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [36/55], Train Loss: 0.0339, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [37/55], Train Loss: 0.0328, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [38/55], Train Loss: 0.0314, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [39/55], Train Loss: 0.0324, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [40/55], Train Loss: 0.0337, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [41/55], Train Loss: 0.0337, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [42/55], Train Loss: 0.0332, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [43/55], Train Loss: 0.0332, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [44/55], Train Loss: 0.0319, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [45/55], Train Loss: 0.0340, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [46/55], Train Loss: 0.0318, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [47/55], Train Loss: 0.0349, Train Acc: 92.07%, Best Acc: 92.61261%\n",
      "Epoch [48/55], Train Loss: 0.0328, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [49/55], Train Loss: 0.0332, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [50/55], Train Loss: 0.0333, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [51/55], Train Loss: 0.0314, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [52/55], Train Loss: 0.0322, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [53/55], Train Loss: 0.0324, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [54/55], Train Loss: 0.0351, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Epoch [55/55], Train Loss: 0.0314, Train Acc: 92.61%, Best Acc: 92.61261%\n",
      "Model 2 training complete with best accuracy: 92.61%\n",
      "\n",
      "Training model 3...\n",
      "Class counts in the subset:\n",
      "Class 0: 516 samples (92.97%)\n",
      "Class 1: 34 samples (6.13%)\n",
      "Class 2: 4 samples (0.72%)\n",
      "Class 4: 1 samples (0.18%)\n",
      "Epoch [1/55], Train Loss: 0.0952, Train Acc: 84.14%, Best Acc: 84.14414%\n",
      "Epoch [2/55], Train Loss: 0.0407, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [3/55], Train Loss: 0.0419, Train Acc: 92.79%, Best Acc: 92.97297%\n",
      "Epoch [4/55], Train Loss: 0.0393, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [5/55], Train Loss: 0.0394, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [6/55], Train Loss: 0.0393, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [7/55], Train Loss: 0.0382, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [8/55], Train Loss: 0.0390, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [9/55], Train Loss: 0.0385, Train Acc: 92.61%, Best Acc: 92.97297%\n",
      "Epoch [10/55], Train Loss: 0.0343, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [11/55], Train Loss: 0.0363, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [12/55], Train Loss: 0.0356, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [13/55], Train Loss: 0.0355, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [14/55], Train Loss: 0.0328, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [15/55], Train Loss: 0.0342, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [16/55], Train Loss: 0.0332, Train Acc: 92.79%, Best Acc: 92.97297%\n",
      "Epoch [17/55], Train Loss: 0.0347, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [18/55], Train Loss: 0.0344, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [19/55], Train Loss: 0.0351, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [20/55], Train Loss: 0.0352, Train Acc: 92.79%, Best Acc: 92.97297%\n",
      "Epoch [21/55], Train Loss: 0.0344, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [22/55], Train Loss: 0.0347, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [23/55], Train Loss: 0.0346, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [24/55], Train Loss: 0.0351, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [25/55], Train Loss: 0.0362, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [26/55], Train Loss: 0.0335, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [27/55], Train Loss: 0.0334, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [28/55], Train Loss: 0.0338, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [29/55], Train Loss: 0.0368, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [30/55], Train Loss: 0.0344, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [31/55], Train Loss: 0.0343, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [32/55], Train Loss: 0.0343, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [33/55], Train Loss: 0.0336, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [34/55], Train Loss: 0.0359, Train Acc: 92.79%, Best Acc: 92.97297%\n",
      "Epoch [35/55], Train Loss: 0.0339, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [36/55], Train Loss: 0.0329, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [37/55], Train Loss: 0.0327, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [38/55], Train Loss: 0.0350, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [39/55], Train Loss: 0.0322, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [40/55], Train Loss: 0.0345, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [41/55], Train Loss: 0.0340, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [42/55], Train Loss: 0.0343, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [43/55], Train Loss: 0.0325, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [44/55], Train Loss: 0.0316, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [45/55], Train Loss: 0.0332, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [46/55], Train Loss: 0.0351, Train Acc: 92.61%, Best Acc: 92.97297%\n",
      "Epoch [47/55], Train Loss: 0.0336, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [48/55], Train Loss: 0.0344, Train Acc: 92.97%, Best Acc: 92.97297%\n",
      "Epoch [49/55], Train Loss: 0.0306, Train Acc: 93.15%, Best Acc: 93.15315%\n",
      "Epoch [50/55], Train Loss: 0.0336, Train Acc: 92.97%, Best Acc: 93.15315%\n",
      "Epoch [51/55], Train Loss: 0.0322, Train Acc: 92.97%, Best Acc: 93.15315%\n",
      "Epoch [52/55], Train Loss: 0.0336, Train Acc: 92.97%, Best Acc: 93.15315%\n",
      "Epoch [53/55], Train Loss: 0.0334, Train Acc: 92.07%, Best Acc: 93.15315%\n",
      "Epoch [54/55], Train Loss: 0.0343, Train Acc: 92.97%, Best Acc: 93.15315%\n",
      "Epoch [55/55], Train Loss: 0.0342, Train Acc: 92.97%, Best Acc: 93.15315%\n",
      "Model 3 training complete with best accuracy: 93.15%\n",
      "\n",
      "Training model 4...\n",
      "Class counts in the subset:\n",
      "Class 0: 513 samples (92.43%)\n",
      "Class 1: 33 samples (5.95%)\n",
      "Class 2: 5 samples (0.90%)\n",
      "Class 3: 2 samples (0.36%)\n",
      "Class 4: 2 samples (0.36%)\n",
      "Epoch [1/55], Train Loss: 0.1073, Train Acc: 81.44%, Best Acc: 81.44144%\n",
      "Epoch [2/55], Train Loss: 0.0440, Train Acc: 92.25%, Best Acc: 92.25225%\n",
      "Epoch [3/55], Train Loss: 0.0390, Train Acc: 92.25%, Best Acc: 92.25225%\n",
      "Epoch [4/55], Train Loss: 0.0381, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [5/55], Train Loss: 0.0367, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [6/55], Train Loss: 0.0374, Train Acc: 92.25%, Best Acc: 92.43243%\n",
      "Epoch [7/55], Train Loss: 0.0358, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [8/55], Train Loss: 0.0350, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [9/55], Train Loss: 0.0355, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [10/55], Train Loss: 0.0365, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [11/55], Train Loss: 0.0358, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [12/55], Train Loss: 0.0382, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [13/55], Train Loss: 0.0353, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [14/55], Train Loss: 0.0343, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [15/55], Train Loss: 0.0342, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [16/55], Train Loss: 0.0361, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [17/55], Train Loss: 0.0344, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [18/55], Train Loss: 0.0347, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [19/55], Train Loss: 0.0338, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [20/55], Train Loss: 0.0342, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [21/55], Train Loss: 0.0339, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [22/55], Train Loss: 0.0336, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [23/55], Train Loss: 0.0333, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [24/55], Train Loss: 0.0339, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [25/55], Train Loss: 0.0340, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [26/55], Train Loss: 0.0347, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [27/55], Train Loss: 0.0332, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [28/55], Train Loss: 0.0346, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [29/55], Train Loss: 0.0339, Train Acc: 92.25%, Best Acc: 92.43243%\n",
      "Epoch [30/55], Train Loss: 0.0317, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [31/55], Train Loss: 0.0338, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [32/55], Train Loss: 0.0314, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [33/55], Train Loss: 0.0326, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [34/55], Train Loss: 0.0331, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [35/55], Train Loss: 0.0319, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [36/55], Train Loss: 0.0318, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [37/55], Train Loss: 0.0332, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [38/55], Train Loss: 0.0318, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [39/55], Train Loss: 0.0348, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [40/55], Train Loss: 0.0326, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [41/55], Train Loss: 0.0328, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [42/55], Train Loss: 0.0320, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [43/55], Train Loss: 0.0308, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [44/55], Train Loss: 0.0343, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [45/55], Train Loss: 0.0325, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [46/55], Train Loss: 0.0321, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [47/55], Train Loss: 0.0336, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [48/55], Train Loss: 0.0324, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [49/55], Train Loss: 0.0319, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [50/55], Train Loss: 0.0322, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [51/55], Train Loss: 0.0321, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [52/55], Train Loss: 0.0334, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [53/55], Train Loss: 0.0327, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [54/55], Train Loss: 0.0316, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Epoch [55/55], Train Loss: 0.0316, Train Acc: 92.43%, Best Acc: 92.43243%\n",
      "Model 4 training complete with best accuracy: 92.43%\n",
      "\n",
      "Training model 5...\n",
      "Class counts in the subset:\n",
      "Class 0: 511 samples (92.07%)\n",
      "Class 1: 35 samples (6.31%)\n",
      "Class 2: 5 samples (0.90%)\n",
      "Class 3: 2 samples (0.36%)\n",
      "Class 4: 2 samples (0.36%)\n",
      "Epoch [1/55], Train Loss: 0.0714, Train Acc: 87.03%, Best Acc: 87.02703%\n",
      "Epoch [2/55], Train Loss: 0.0405, Train Acc: 91.71%, Best Acc: 91.71171%\n",
      "Epoch [3/55], Train Loss: 0.0413, Train Acc: 91.89%, Best Acc: 91.89189%\n",
      "Epoch [4/55], Train Loss: 0.0446, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [5/55], Train Loss: 0.0376, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [6/55], Train Loss: 0.0370, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [7/55], Train Loss: 0.0390, Train Acc: 91.89%, Best Acc: 92.07207%\n",
      "Epoch [8/55], Train Loss: 0.0367, Train Acc: 91.89%, Best Acc: 92.07207%\n",
      "Epoch [9/55], Train Loss: 0.0369, Train Acc: 91.89%, Best Acc: 92.07207%\n",
      "Epoch [10/55], Train Loss: 0.0382, Train Acc: 91.89%, Best Acc: 92.07207%\n",
      "Epoch [11/55], Train Loss: 0.0368, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [12/55], Train Loss: 0.0363, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [13/55], Train Loss: 0.0347, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [14/55], Train Loss: 0.0364, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [15/55], Train Loss: 0.0373, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [16/55], Train Loss: 0.0381, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [17/55], Train Loss: 0.0369, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [18/55], Train Loss: 0.0370, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [19/55], Train Loss: 0.0350, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [20/55], Train Loss: 0.0367, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [21/55], Train Loss: 0.0334, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [22/55], Train Loss: 0.0341, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [23/55], Train Loss: 0.0357, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [24/55], Train Loss: 0.0346, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [25/55], Train Loss: 0.0332, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [26/55], Train Loss: 0.0348, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [27/55], Train Loss: 0.0343, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [28/55], Train Loss: 0.0346, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [29/55], Train Loss: 0.0358, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [30/55], Train Loss: 0.0352, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [31/55], Train Loss: 0.0343, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [32/55], Train Loss: 0.0345, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [33/55], Train Loss: 0.0327, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [34/55], Train Loss: 0.0331, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [35/55], Train Loss: 0.0339, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [36/55], Train Loss: 0.0343, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [37/55], Train Loss: 0.0354, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [38/55], Train Loss: 0.0338, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [39/55], Train Loss: 0.0342, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [40/55], Train Loss: 0.0338, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [41/55], Train Loss: 0.0339, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [42/55], Train Loss: 0.0335, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [43/55], Train Loss: 0.0334, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [44/55], Train Loss: 0.0348, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [45/55], Train Loss: 0.0324, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [46/55], Train Loss: 0.0328, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [47/55], Train Loss: 0.0333, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [48/55], Train Loss: 0.0338, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [49/55], Train Loss: 0.0335, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [50/55], Train Loss: 0.0343, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [51/55], Train Loss: 0.0339, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [52/55], Train Loss: 0.0329, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [53/55], Train Loss: 0.0338, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [54/55], Train Loss: 0.0325, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Epoch [55/55], Train Loss: 0.0330, Train Acc: 92.07%, Best Acc: 92.07207%\n",
      "Model 5 training complete with best accuracy: 92.07%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import copy\n",
    "\n",
    "models = []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for bagging_idx in range(bagging):\n",
    "    print(f\"Training model {bagging_idx + 1}...\")\n",
    "\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, num_classes, debug=False)\n",
    "    torch.manual_seed(bagging_idx)\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = FocalLoss(alpha=alpha_np, gamma=2.5, label_smoothing=0.1)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-7)\n",
    "    \n",
    "    random.seed(bagging_idx * 55)\n",
    "    indices = random.sample(range(len(train_dataset)), subset_size)\n",
    "    subset_labels = [train_dataset[i][1].item() for i in indices]\n",
    "\n",
    "    class_counts = Counter(subset_labels)\n",
    "    total_samples = len(subset_labels)\n",
    "    percentages = {cls: (count / total_samples) * 100 for cls, count in class_counts.items()}\n",
    "\n",
    "    print(\"Class counts in the subset:\")\n",
    "    for cls in sorted(class_counts.keys()):\n",
    "        print(f\"Class {cls}: {class_counts[cls]} samples ({percentages[cls]:.2f}%)\")\n",
    "\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(indices)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "\n",
    "    best_accuracy = 0\n",
    "    best_model_state = None  \n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_accuracy = 100 * correct / total\n",
    "\n",
    "        if train_accuracy > best_accuracy:\n",
    "            best_accuracy = train_accuracy\n",
    "            best_model_state = copy.deepcopy(model.state_dict())  \n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, Best Acc: {best_accuracy:.5f}%')\n",
    "\n",
    "    models.append(best_model_state)\n",
    "    print(f\"Model {bagging_idx + 1} training complete with best accuracy: {best_accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "296a05a7-214b-4df2-af17-e8f92d3e93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, model_state in enumerate(models):\n",
    "    torch.save(model_state, f\"bagged_model_{idx + 1}.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "69abf19a-bbd6-4e75-affe-7907e1848406",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_2024_test_cleaned = pd.read_csv('X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "fc3d2a2b-df9e-4422-b17a-e5dfc80e2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_numeric = merged_data_2024_test_cleaned.select_dtypes(include=[float, int])\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_numeric.values, dtype=torch.float32)\n",
    "\n",
    "def create_sequences_test(X, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(seq_length, len(X)):\n",
    "        X_seq = X[i-seq_length:i]\n",
    "        sequences.append(X_seq)\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "sequence_length = 5\n",
    "X_test_sequences = create_sequences_test(X_test_tensor, sequence_length)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_sequences)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "62a1b9e4-d324-45f1-bf7d-bca773321ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sn/shfjhkz51018ys93dvxxl92w0000gn/T/ipykernel_42646/3099917206.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(f'bagged_model_{bagging_idx + 1}.pth', map_location=device), strict=False)\n"
     ]
    }
   ],
   "source": [
    "sequence_predictions = []\n",
    "\n",
    "for bagging_idx in range(bagging):\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, num_classes)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f'bagged_model_{bagging_idx + 1}.pth', map_location=device), strict=False)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    model_sequence_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch[0].to(device)  \n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            model_sequence_predictions.extend(predicted.cpu().numpy())\n",
    "    \n",
    "    sequence_predictions.append(model_sequence_predictions)\n",
    "\n",
    "\n",
    "sequence_predictions = np.array(sequence_predictions)\n",
    "final_predictions = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=sequence_predictions)\n",
    "\n",
    "individual_predictions = []\n",
    "for i in range(len(X_test_numeric) - sequence_length):\n",
    "    individual_predictions.append(final_predictions[i])\n",
    "\n",
    "for i in range(sequence_length):\n",
    "    individual_predictions.insert(0, individual_predictions[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "ea9c2cea-e1a9-419d-adf8-6e0f3e80c6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID Predicted_AQI\n",
      "0   1      Moderate\n",
      "1   2      Moderate\n",
      "2   3      Moderate\n",
      "3   4      Moderate\n",
      "4   5      Moderate\n",
      "Total predictions: 133\n",
      "Original data points: 133\n"
     ]
    }
   ],
   "source": [
    "label_mapping_legend = {'Good': 0, 'Moderate': 1, 'Poor': 2, 'Severe': 3, 'Unhealthy': 4}\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping_legend.items()}\n",
    "test_predictions_labels = pd.Series([reverse_label_mapping[pred] for pred in individual_predictions], name='Predicted_AQI')\n",
    "\n",
    "ID_column = pd.Series(range(1, len(individual_predictions) + 1), name='ID')\n",
    "predictions_df = pd.concat([ID_column, test_predictions_labels], axis=1)\n",
    "\n",
    "predictions_df.to_csv('test_predictions_with_labels_bagging.csv', index=False)\n",
    "print(predictions_df.head())\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(f\"Original data points: {len(X_test_numeric)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "01c5ff60-f4ac-4795-b0a7-ead79b4b1fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predicted_AQI\n",
       "Good        82\n",
       "Moderate    51\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##changing the distribution made the model's acuracy lower. could change the sample size of the bagging \n",
    "unique_value_counts = predictions_df['Predicted_AQI'].value_counts()\n",
    "unique_value_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a5962-b861-48de-93e4-8a38d61de5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78e331-2ebe-42f4-b0f5-2447dc4706ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
