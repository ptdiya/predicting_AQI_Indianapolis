{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0fd614-06e9-4bf3-8a84-2f5c0e0cf5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_val = pd.read_csv('X_val.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "y_val = pd.read_csv('y_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e959fbe-6030-48a8-9316-132aac6e6b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Traffic_Volume</th>\n",
       "      <th>tavg</th>\n",
       "      <th>tmin</th>\n",
       "      <th>tmax</th>\n",
       "      <th>prcp</th>\n",
       "      <th>snow</th>\n",
       "      <th>wdir</th>\n",
       "      <th>wspd</th>\n",
       "      <th>pres</th>\n",
       "      <th>year</th>\n",
       "      <th>...</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "      <th>week_number_sin</th>\n",
       "      <th>week_number_cos</th>\n",
       "      <th>quarter_sin</th>\n",
       "      <th>quarter_cos</th>\n",
       "      <th>four_month_sin</th>\n",
       "      <th>four_month_cos</th>\n",
       "      <th>half_year_sin</th>\n",
       "      <th>half_year_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.277074</td>\n",
       "      <td>-0.101973</td>\n",
       "      <td>-0.381232</td>\n",
       "      <td>-0.134296</td>\n",
       "      <td>1.188318</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.603098</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>-1.822248</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>6.432491e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.499842</td>\n",
       "      <td>-1.172810</td>\n",
       "      <td>-1.231675</td>\n",
       "      <td>-1.351007</td>\n",
       "      <td>-0.306376</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-2.009760</td>\n",
       "      <td>0.485982</td>\n",
       "      <td>0.014492</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>6.432491e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.332017</td>\n",
       "      <td>-1.617308</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.712218</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.018358</td>\n",
       "      <td>-0.774021</td>\n",
       "      <td>1.760901</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.266043</td>\n",
       "      <td>-1.475877</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.189412</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.255540</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>0.526370</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.734597</td>\n",
       "      <td>-1.081889</td>\n",
       "      <td>-1.750238</td>\n",
       "      <td>-1.246446</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.579803</td>\n",
       "      <td>3.157190</td>\n",
       "      <td>-0.994210</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.596741</td>\n",
       "      <td>-2.152726</td>\n",
       "      <td>-1.978405</td>\n",
       "      <td>-2.396618</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.893057</td>\n",
       "      <td>0.670783</td>\n",
       "      <td>0.511315</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.729602</td>\n",
       "      <td>-2.425486</td>\n",
       "      <td>-2.382884</td>\n",
       "      <td>-2.339584</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.684221</td>\n",
       "      <td>-0.354020</td>\n",
       "      <td>1.685625</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-0.084275</td>\n",
       "      <td>-1.890068</td>\n",
       "      <td>-1.750238</td>\n",
       "      <td>-1.293973</td>\n",
       "      <td>0.239762</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.464375</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>1.309243</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.569726</td>\n",
       "      <td>-0.980867</td>\n",
       "      <td>-1.573926</td>\n",
       "      <td>-1.132379</td>\n",
       "      <td>1.087714</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.913940</td>\n",
       "      <td>1.157984</td>\n",
       "      <td>1.203857</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-1.152162</td>\n",
       "      <td>-1.910272</td>\n",
       "      <td>-1.812465</td>\n",
       "      <td>-1.864307</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.934824</td>\n",
       "      <td>0.368382</td>\n",
       "      <td>2.619050</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050584</td>\n",
       "      <td>-1.970886</td>\n",
       "      <td>-1.978405</td>\n",
       "      <td>-1.607657</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.203331</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>2.363111</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.054306</td>\n",
       "      <td>-0.940458</td>\n",
       "      <td>-0.723483</td>\n",
       "      <td>-0.818696</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.245666</td>\n",
       "      <td>0.435582</td>\n",
       "      <td>0.390873</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.257369</td>\n",
       "      <td>-0.960663</td>\n",
       "      <td>-0.941280</td>\n",
       "      <td>-1.027818</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.039242</td>\n",
       "      <td>-0.774021</td>\n",
       "      <td>-0.572663</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.318988</td>\n",
       "      <td>-1.193014</td>\n",
       "      <td>-0.941280</td>\n",
       "      <td>-1.503095</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.498112</td>\n",
       "      <td>-0.354020</td>\n",
       "      <td>0.420984</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.077282</td>\n",
       "      <td>-1.435468</td>\n",
       "      <td>-1.459842</td>\n",
       "      <td>-1.607657</td>\n",
       "      <td>-0.191399</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.456345</td>\n",
       "      <td>1.157984</td>\n",
       "      <td>1.143636</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.110522</td>\n",
       "      <td>-1.718330</td>\n",
       "      <td>-1.688010</td>\n",
       "      <td>-1.398534</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.936667</td>\n",
       "      <td>-0.118819</td>\n",
       "      <td>-0.422110</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>2.393157e-01</td>\n",
       "      <td>0.970942</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.199429</td>\n",
       "      <td>-1.587001</td>\n",
       "      <td>-1.231675</td>\n",
       "      <td>-1.864307</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.018358</td>\n",
       "      <td>0.973184</td>\n",
       "      <td>-0.994210</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.108524</td>\n",
       "      <td>-1.344548</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.027818</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.015379</td>\n",
       "      <td>0.301182</td>\n",
       "      <td>-0.045729</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.267358</td>\n",
       "      <td>-0.839436</td>\n",
       "      <td>-1.407986</td>\n",
       "      <td>-0.980290</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.861731</td>\n",
       "      <td>1.645186</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-0.979342</td>\n",
       "      <td>-1.940579</td>\n",
       "      <td>-1.812465</td>\n",
       "      <td>-2.073429</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.655307</td>\n",
       "      <td>0.250782</td>\n",
       "      <td>2.287835</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-1.269040</td>\n",
       "      <td>-1.869864</td>\n",
       "      <td>-1.688010</td>\n",
       "      <td>-1.968868</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.195300</td>\n",
       "      <td>-0.421220</td>\n",
       "      <td>2.634105</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.226401</td>\n",
       "      <td>-1.930477</td>\n",
       "      <td>-1.926549</td>\n",
       "      <td>-1.607657</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.130806</td>\n",
       "      <td>0.301182</td>\n",
       "      <td>1.550127</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>-0.020342</td>\n",
       "      <td>-1.485979</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.560129</td>\n",
       "      <td>-0.306376</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.882615</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>-0.331779</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>3.546049e-01</td>\n",
       "      <td>0.935016</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.229398</td>\n",
       "      <td>-1.496081</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.351007</td>\n",
       "      <td>-0.335120</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.183015</td>\n",
       "      <td>1.090784</td>\n",
       "      <td>-1.129707</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.105527</td>\n",
       "      <td>-1.900170</td>\n",
       "      <td>-2.092489</td>\n",
       "      <td>-2.025901</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.289845</td>\n",
       "      <td>0.183582</td>\n",
       "      <td>1.038249</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.334289</td>\n",
       "      <td>-2.486100</td>\n",
       "      <td>-2.559196</td>\n",
       "      <td>-2.235023</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.039242</td>\n",
       "      <td>-1.076422</td>\n",
       "      <td>2.302890</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-1.581714</td>\n",
       "      <td>-1.940579</td>\n",
       "      <td>-1.978405</td>\n",
       "      <td>-1.759745</td>\n",
       "      <td>-0.335120</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.224783</td>\n",
       "      <td>0.183582</td>\n",
       "      <td>1.203857</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.217411</td>\n",
       "      <td>-1.566797</td>\n",
       "      <td>-2.154717</td>\n",
       "      <td>-1.759745</td>\n",
       "      <td>-0.306376</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.540448</td>\n",
       "      <td>-0.051619</td>\n",
       "      <td>1.068359</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.739592</td>\n",
       "      <td>-2.294157</td>\n",
       "      <td>-2.382884</td>\n",
       "      <td>-1.864307</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.381409</td>\n",
       "      <td>-0.841222</td>\n",
       "      <td>1.053304</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.096537</td>\n",
       "      <td>-1.597103</td>\n",
       "      <td>-1.573926</td>\n",
       "      <td>-1.351007</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.966149</td>\n",
       "      <td>-0.606021</td>\n",
       "      <td>-0.346834</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>0.885456</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.670938</td>\n",
       "      <td>-1.536490</td>\n",
       "      <td>-1.522070</td>\n",
       "      <td>-1.293973</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.017790</td>\n",
       "      <td>-0.841222</td>\n",
       "      <td>0.646812</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>-1.017302</td>\n",
       "      <td>-0.738414</td>\n",
       "      <td>-0.827196</td>\n",
       "      <td>-0.238857</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.339074</td>\n",
       "      <td>0.435582</td>\n",
       "      <td>0.165044</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>-0.908416</td>\n",
       "      <td>-0.526267</td>\n",
       "      <td>-1.055363</td>\n",
       "      <td>-0.609573</td>\n",
       "      <td>3.890265</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.717390</td>\n",
       "      <td>0.183582</td>\n",
       "      <td>0.134934</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.173456</td>\n",
       "      <td>-1.688023</td>\n",
       "      <td>-1.459842</td>\n",
       "      <td>-1.921340</td>\n",
       "      <td>1.375155</td>\n",
       "      <td>2.212056</td>\n",
       "      <td>-1.821808</td>\n",
       "      <td>2.485188</td>\n",
       "      <td>1.068359</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>-0.753577</td>\n",
       "      <td>-2.001193</td>\n",
       "      <td>-1.812465</td>\n",
       "      <td>-2.235023</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>13.960551</td>\n",
       "      <td>1.456913</td>\n",
       "      <td>-0.118819</td>\n",
       "      <td>1.580238</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>-0.614722</td>\n",
       "      <td>-2.253748</td>\n",
       "      <td>-2.445112</td>\n",
       "      <td>-2.292057</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>11.610852</td>\n",
       "      <td>0.621570</td>\n",
       "      <td>-0.354020</td>\n",
       "      <td>2.317945</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>-0.820507</td>\n",
       "      <td>-1.859761</td>\n",
       "      <td>-1.926549</td>\n",
       "      <td>-1.398534</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>11.610852</td>\n",
       "      <td>-0.182447</td>\n",
       "      <td>-0.538821</td>\n",
       "      <td>1.429685</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.680647e-01</td>\n",
       "      <td>0.822984</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>-1.019300</td>\n",
       "      <td>-1.698126</td>\n",
       "      <td>-1.864321</td>\n",
       "      <td>-2.025901</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>10.044386</td>\n",
       "      <td>0.830406</td>\n",
       "      <td>-0.354020</td>\n",
       "      <td>1.038249</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>-0.395950</td>\n",
       "      <td>-1.738535</td>\n",
       "      <td>-2.092489</td>\n",
       "      <td>-0.980290</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>7.694687</td>\n",
       "      <td>-0.245098</td>\n",
       "      <td>-0.051619</td>\n",
       "      <td>0.285486</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>-0.647688</td>\n",
       "      <td>-0.748516</td>\n",
       "      <td>-0.547172</td>\n",
       "      <td>-0.980290</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>6.128221</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>1.040384</td>\n",
       "      <td>-0.933989</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>-0.091268</td>\n",
       "      <td>-1.102094</td>\n",
       "      <td>-1.003507</td>\n",
       "      <td>-1.455568</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>2.212056</td>\n",
       "      <td>0.569361</td>\n",
       "      <td>0.250782</td>\n",
       "      <td>-0.301668</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>-0.376970</td>\n",
       "      <td>-1.001072</td>\n",
       "      <td>-0.827196</td>\n",
       "      <td>-0.980290</td>\n",
       "      <td>0.024182</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.256108</td>\n",
       "      <td>0.855584</td>\n",
       "      <td>-1.280259</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>-0.875450</td>\n",
       "      <td>-1.455672</td>\n",
       "      <td>-1.573926</td>\n",
       "      <td>-1.607657</td>\n",
       "      <td>-0.335120</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.174985</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>1.128580</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>-0.685648</td>\n",
       "      <td>-2.041602</td>\n",
       "      <td>-2.040633</td>\n",
       "      <td>-2.235023</td>\n",
       "      <td>-0.306376</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.174985</td>\n",
       "      <td>-0.908422</td>\n",
       "      <td>1.489906</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6.631227e-01</td>\n",
       "      <td>0.748511</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>-0.121236</td>\n",
       "      <td>-2.102215</td>\n",
       "      <td>-1.978405</td>\n",
       "      <td>-1.816779</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.955708</td>\n",
       "      <td>-1.630824</td>\n",
       "      <td>1.655514</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.485107e-01</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>-0.298052</td>\n",
       "      <td>-1.415263</td>\n",
       "      <td>-1.522070</td>\n",
       "      <td>-0.980290</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.913372</td>\n",
       "      <td>-0.169220</td>\n",
       "      <td>1.745846</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.485107e-01</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.214414</td>\n",
       "      <td>-0.263609</td>\n",
       "      <td>-0.142693</td>\n",
       "      <td>-0.181823</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.130238</td>\n",
       "      <td>1.947587</td>\n",
       "      <td>0.134934</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.485107e-01</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>-1.132182</td>\n",
       "      <td>-0.233302</td>\n",
       "      <td>-1.055363</td>\n",
       "      <td>-0.238857</td>\n",
       "      <td>5.614912</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.966149</td>\n",
       "      <td>1.393185</td>\n",
       "      <td>-1.174873</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.485107e-01</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>-0.420924</td>\n",
       "      <td>-1.667819</td>\n",
       "      <td>-1.750238</td>\n",
       "      <td>-1.664690</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.579803</td>\n",
       "      <td>0.973184</td>\n",
       "      <td>0.827475</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.485107e-01</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>-0.868457</td>\n",
       "      <td>-1.597103</td>\n",
       "      <td>-1.459842</td>\n",
       "      <td>-1.607657</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.091451</td>\n",
       "      <td>0.973184</td>\n",
       "      <td>1.580238</td>\n",
       "      <td>2022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>7.485107e-01</td>\n",
       "      <td>0.663123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50 rows Ã— 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Traffic_Volume      tavg      tmin      tmax      prcp       snow  \\\n",
       "0        -0.277074 -0.101973 -0.381232 -0.134296  1.188318  -0.137643   \n",
       "1        -0.499842 -1.172810 -1.231675 -1.351007 -0.306376  -0.137643   \n",
       "2        -0.332017 -1.617308 -1.345759 -1.712218 -0.378236  -0.137643   \n",
       "3        -1.266043 -1.475877 -1.345759 -1.189412 -0.378236  -0.137643   \n",
       "4        -0.734597 -1.081889 -1.750238 -1.246446 -0.378236  -0.137643   \n",
       "5        -0.596741 -2.152726 -1.978405 -2.396618 -0.378236  -0.137643   \n",
       "6        -0.729602 -2.425486 -2.382884 -2.339584 -0.378236  -0.137643   \n",
       "7        -0.084275 -1.890068 -1.750238 -1.293973  0.239762  -0.137643   \n",
       "8        -1.569726 -0.980867 -1.573926 -1.132379  1.087714  -0.137643   \n",
       "9        -1.152162 -1.910272 -1.812465 -1.864307 -0.378236  -0.137643   \n",
       "10        0.050584 -1.970886 -1.978405 -1.607657 -0.378236  -0.137643   \n",
       "11       -0.054306 -0.940458 -0.723483 -0.818696 -0.378236  -0.137643   \n",
       "12        0.257369 -0.960663 -0.941280 -1.027818 -0.378236  -0.137643   \n",
       "13       -1.318988 -1.193014 -0.941280 -1.503095 -0.378236  -0.137643   \n",
       "14       -0.077282 -1.435468 -1.459842 -1.607657 -0.191399  -0.137643   \n",
       "15        0.110522 -1.718330 -1.688010 -1.398534 -0.378236  -0.137643   \n",
       "16        0.199429 -1.587001 -1.231675 -1.864307 -0.378236  -0.137643   \n",
       "17        0.108524 -1.344548 -1.345759 -1.027818 -0.378236  -0.137643   \n",
       "18        0.267358 -0.839436 -1.407986 -0.980290 -0.378236  -0.137643   \n",
       "19       -0.979342 -1.940579 -1.812465 -2.073429 -0.378236  -0.137643   \n",
       "20       -1.269040 -1.869864 -1.688010 -1.968868 -0.378236  -0.137643   \n",
       "21        0.226401 -1.930477 -1.926549 -1.607657 -0.378236  -0.137643   \n",
       "22       -0.020342 -1.485979 -1.345759 -1.560129 -0.306376  -0.137643   \n",
       "23        0.229398 -1.496081 -1.345759 -1.351007 -0.335120  -0.137643   \n",
       "24        0.105527 -1.900170 -2.092489 -2.025901 -0.378236  -0.137643   \n",
       "25        0.334289 -2.486100 -2.559196 -2.235023 -0.378236  -0.137643   \n",
       "26       -1.581714 -1.940579 -1.978405 -1.759745 -0.335120  -0.137643   \n",
       "27        0.217411 -1.566797 -2.154717 -1.759745 -0.306376  -0.137643   \n",
       "28       -0.739592 -2.294157 -2.382884 -1.864307 -0.378236  -0.137643   \n",
       "29        0.096537 -1.597103 -1.573926 -1.351007 -0.378236  -0.137643   \n",
       "30        0.670938 -1.536490 -1.522070 -1.293973 -0.378236  -0.137643   \n",
       "31       -1.017302 -0.738414 -0.827196 -0.238857 -0.378236  -0.137643   \n",
       "32       -0.908416 -0.526267 -1.055363 -0.609573  3.890265  -0.137643   \n",
       "33        0.173456 -1.688023 -1.459842 -1.921340  1.375155   2.212056   \n",
       "34       -0.753577 -2.001193 -1.812465 -2.235023 -0.378236  13.960551   \n",
       "35       -0.614722 -2.253748 -2.445112 -2.292057 -0.378236  11.610852   \n",
       "36       -0.820507 -1.859761 -1.926549 -1.398534 -0.378236  11.610852   \n",
       "37       -1.019300 -1.698126 -1.864321 -2.025901 -0.378236  10.044386   \n",
       "38       -0.395950 -1.738535 -2.092489 -0.980290 -0.378236   7.694687   \n",
       "39       -0.647688 -0.748516 -0.547172 -0.980290 -0.378236   6.128221   \n",
       "40       -0.091268 -1.102094 -1.003507 -1.455568 -0.378236   2.212056   \n",
       "41       -0.376970 -1.001072 -0.827196 -0.980290  0.024182  -0.137643   \n",
       "42       -0.875450 -1.455672 -1.573926 -1.607657 -0.335120  -0.137643   \n",
       "43       -0.685648 -2.041602 -2.040633 -2.235023 -0.306376  -0.137643   \n",
       "44       -0.121236 -2.102215 -1.978405 -1.816779 -0.378236  -0.137643   \n",
       "45       -0.298052 -1.415263 -1.522070 -0.980290 -0.378236  -0.137643   \n",
       "46        0.214414 -0.263609 -0.142693 -0.181823 -0.378236  -0.137643   \n",
       "47       -1.132182 -0.233302 -1.055363 -0.238857  5.614912  -0.137643   \n",
       "48       -0.420924 -1.667819 -1.750238 -1.664690 -0.378236  -0.137643   \n",
       "49       -0.868457 -1.597103 -1.459842 -1.607657 -0.378236  -0.137643   \n",
       "\n",
       "        wdir      wspd      pres  year  ...  month_sin  month_cos  \\\n",
       "0   1.603098  0.133181 -1.822248  2022  ...   0.500000   0.866025   \n",
       "1  -2.009760  0.485982  0.014492  2022  ...   0.500000   0.866025   \n",
       "2   1.018358 -0.774021  1.760901  2022  ...   0.500000   0.866025   \n",
       "3  -0.255540  0.737983  0.526370  2022  ...   0.500000   0.866025   \n",
       "4   0.579803  3.157190 -0.994210  2022  ...   0.500000   0.866025   \n",
       "5   0.893057  0.670783  0.511315  2022  ...   0.500000   0.866025   \n",
       "6   0.684221 -0.354020  1.685625  2022  ...   0.500000   0.866025   \n",
       "7  -0.464375  0.737983  1.309243  2022  ...   0.500000   0.866025   \n",
       "8   0.913940  1.157984  1.203857  2022  ...   0.500000   0.866025   \n",
       "9   0.934824  0.368382  2.619050  2022  ...   0.500000   0.866025   \n",
       "10 -0.203331  0.133181  2.363111  2022  ...   0.500000   0.866025   \n",
       "11  0.245666  0.435582  0.390873  2022  ...   0.500000   0.866025   \n",
       "12  1.039242 -0.774021 -0.572663  2022  ...   0.500000   0.866025   \n",
       "13 -1.498112 -0.354020  0.420984  2022  ...   0.500000   0.866025   \n",
       "14 -1.456345  1.157984  1.143636  2022  ...   0.500000   0.866025   \n",
       "15 -1.936667 -0.118819 -0.422110  2022  ...   0.500000   0.866025   \n",
       "16  1.018358  0.973184 -0.994210  2022  ...   0.500000   0.866025   \n",
       "17 -0.015379  0.301182 -0.045729  2022  ...   0.500000   0.866025   \n",
       "18  0.861731  1.645186 -0.000563  2022  ...   0.500000   0.866025   \n",
       "19  1.655307  0.250782  2.287835  2022  ...   0.500000   0.866025   \n",
       "20 -1.195300 -0.421220  2.634105  2022  ...   0.500000   0.866025   \n",
       "21  0.130806  0.301182  1.550127  2022  ...   0.500000   0.866025   \n",
       "22  0.882615  0.737983 -0.331779  2022  ...   0.500000   0.866025   \n",
       "23  0.183015  1.090784 -1.129707  2022  ...   0.500000   0.866025   \n",
       "24  1.289845  0.183582  1.038249  2022  ...   0.500000   0.866025   \n",
       "25  1.039242 -1.076422  2.302890  2022  ...   0.500000   0.866025   \n",
       "26  0.224783  0.183582  1.203857  2022  ...   0.500000   0.866025   \n",
       "27  1.540448 -0.051619  1.068359  2022  ...   0.500000   0.866025   \n",
       "28  0.381409 -0.841222  1.053304  2022  ...   0.500000   0.866025   \n",
       "29  0.966149 -0.606021 -0.346834  2022  ...   0.500000   0.866025   \n",
       "30 -1.017790 -0.841222  0.646812  2022  ...   0.500000   0.866025   \n",
       "31 -0.339074  0.435582  0.165044  2022  ...   0.866025   0.500000   \n",
       "32 -1.717390  0.183582  0.134934  2022  ...   0.866025   0.500000   \n",
       "33 -1.821808  2.485188  1.068359  2022  ...   0.866025   0.500000   \n",
       "34  1.456913 -0.118819  1.580238  2022  ...   0.866025   0.500000   \n",
       "35  0.621570 -0.354020  2.317945  2022  ...   0.866025   0.500000   \n",
       "36 -0.182447 -0.538821  1.429685  2022  ...   0.866025   0.500000   \n",
       "37  0.830406 -0.354020  1.038249  2022  ...   0.866025   0.500000   \n",
       "38 -0.245098 -0.051619  0.285486  2022  ...   0.866025   0.500000   \n",
       "39  0.370968  1.040384 -0.933989  2022  ...   0.866025   0.500000   \n",
       "40  0.569361  0.250782 -0.301668  2022  ...   0.866025   0.500000   \n",
       "41  0.256108  0.855584 -1.280259  2022  ...   0.866025   0.500000   \n",
       "42  1.174985  0.737983  1.128580  2022  ...   0.866025   0.500000   \n",
       "43  1.174985 -0.908422  1.489906  2022  ...   0.866025   0.500000   \n",
       "44  0.955708 -1.630824  1.655514  2022  ...   0.866025   0.500000   \n",
       "45 -0.913372 -0.169220  1.745846  2022  ...   0.866025   0.500000   \n",
       "46 -0.130238  1.947587  0.134934  2022  ...   0.866025   0.500000   \n",
       "47  0.966149  1.393185 -1.174873  2022  ...   0.866025   0.500000   \n",
       "48  0.579803  0.973184  0.827475  2022  ...   0.866025   0.500000   \n",
       "49  1.091451  0.973184  1.580238  2022  ...   0.866025   0.500000   \n",
       "\n",
       "    week_number_sin  week_number_cos  quarter_sin   quarter_cos  \\\n",
       "0      6.432491e-16         1.000000          1.0  6.123234e-17   \n",
       "1      6.432491e-16         1.000000          1.0  6.123234e-17   \n",
       "2      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "3      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "4      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "5      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "6      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "7      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "8      1.205367e-01         0.992709          1.0  6.123234e-17   \n",
       "9      2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "10     2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "11     2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "12     2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "13     2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "14     2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "15     2.393157e-01         0.970942          1.0  6.123234e-17   \n",
       "16     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "17     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "18     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "19     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "20     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "21     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "22     3.546049e-01         0.935016          1.0  6.123234e-17   \n",
       "23     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "24     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "25     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "26     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "27     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "28     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "29     4.647232e-01         0.885456          1.0  6.123234e-17   \n",
       "30     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "31     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "32     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "33     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "34     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "35     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "36     5.680647e-01         0.822984          1.0  6.123234e-17   \n",
       "37     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "38     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "39     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "40     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "41     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "42     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "43     6.631227e-01         0.748511          1.0  6.123234e-17   \n",
       "44     7.485107e-01         0.663123          1.0  6.123234e-17   \n",
       "45     7.485107e-01         0.663123          1.0  6.123234e-17   \n",
       "46     7.485107e-01         0.663123          1.0  6.123234e-17   \n",
       "47     7.485107e-01         0.663123          1.0  6.123234e-17   \n",
       "48     7.485107e-01         0.663123          1.0  6.123234e-17   \n",
       "49     7.485107e-01         0.663123          1.0  6.123234e-17   \n",
       "\n",
       "    four_month_sin  four_month_cos  half_year_sin  half_year_cos  \n",
       "0         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "1         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "2         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "3         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "4         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "5         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "6         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "7         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "8         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "9         0.866025            -0.5   1.224647e-16           -1.0  \n",
       "10        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "11        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "12        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "13        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "14        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "15        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "16        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "17        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "18        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "19        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "20        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "21        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "22        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "23        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "24        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "25        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "26        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "27        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "28        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "29        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "30        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "31        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "32        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "33        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "34        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "35        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "36        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "37        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "38        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "39        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "40        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "41        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "42        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "43        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "44        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "45        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "46        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "47        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "48        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "49        0.866025            -0.5   1.224647e-16           -1.0  \n",
       "\n",
       "[50 rows x 35 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2de9504f-8a3f-462a-bf96-5768dabef409",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "X_train_numeric = X_train.select_dtypes(include=[float, int])\n",
    "X_val_numeric = X_val.select_dtypes(include=[float, int])\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_numeric.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)  \n",
    "X_val_tensor = torch.tensor(X_val_numeric.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7803604b-d213-425d-9918-660743c6415f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_labels = y_train_tensor\n",
    "y_val_labels = y_val_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab796678-0292-4d0a-abf8-0d587b09a10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([705, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77093a83-c215-4862-9fd4-7a434bfa94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_combined = torch.cat((X_train_tensor, y_train_tensor), dim=1)\n",
    "val_combined = torch.cat((X_val_tensor, y_val_tensor), dim=1)\n",
    "full_dataset = torch.cat((train_combined, val_combined), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3ccb29f-a881-4518-965d-9632e6c456a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, targets):\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc2357eb-e4ee-4735-be0a-777eaa6f66eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_sequences shape: torch.Size([878, 4, 35])\n",
      "y_sequences shape: torch.Size([878])\n"
     ]
    }
   ],
   "source": [
    "def create_subsequences(full_data, seq_length):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    X_data = full_data[:, :-1]  \n",
    "    y_data = full_data[:, -1]  \n",
    "\n",
    "    for i in range(seq_length, len(full_data)):\n",
    "        X_seq = X_data[i - seq_length:i]\n",
    "        y_seq = y_data[i] \n",
    "        sequences.append(X_seq)\n",
    "        targets.append(y_seq)\n",
    "        \n",
    "    return torch.stack(sequences), torch.tensor(targets).long()\n",
    "\n",
    "sequence_length = 4\n",
    "\n",
    "X_sequences, y_sequences = create_subsequences(full_dataset, sequence_length)\n",
    "\n",
    "print(f\"X_sequences shape: {X_sequences.shape}\")\n",
    "print(f\"y_sequences shape: {y_sequences.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84f8ce1e-e779-4b9e-a451-cc3de66b24ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.277074</td>\n",
       "      <td>-0.101973</td>\n",
       "      <td>-0.381232</td>\n",
       "      <td>-0.134296</td>\n",
       "      <td>1.188318</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.603098</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>-1.822248</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>6.432490e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.499842</td>\n",
       "      <td>-1.172810</td>\n",
       "      <td>-1.231675</td>\n",
       "      <td>-1.351007</td>\n",
       "      <td>-0.306376</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-2.009760</td>\n",
       "      <td>0.485982</td>\n",
       "      <td>0.014492</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>6.432490e-16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.332017</td>\n",
       "      <td>-1.617308</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.712218</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.018358</td>\n",
       "      <td>-0.774021</td>\n",
       "      <td>1.760901</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.266043</td>\n",
       "      <td>-1.475877</td>\n",
       "      <td>-1.345759</td>\n",
       "      <td>-1.189412</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.255540</td>\n",
       "      <td>0.737983</td>\n",
       "      <td>0.526370</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.734597</td>\n",
       "      <td>-1.081890</td>\n",
       "      <td>-1.750238</td>\n",
       "      <td>-1.246446</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.579803</td>\n",
       "      <td>3.157190</td>\n",
       "      <td>-0.994210</td>\n",
       "      <td>2022.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>1.205367e-01</td>\n",
       "      <td>0.992709</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>1.530042</td>\n",
       "      <td>0.726410</td>\n",
       "      <td>0.946289</td>\n",
       "      <td>0.550104</td>\n",
       "      <td>-0.335120</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.611129</td>\n",
       "      <td>1.275585</td>\n",
       "      <td>-1.566309</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>1.462113</td>\n",
       "      <td>0.675899</td>\n",
       "      <td>0.655894</td>\n",
       "      <td>0.654665</td>\n",
       "      <td>-0.306376</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>0.861731</td>\n",
       "      <td>0.133181</td>\n",
       "      <td>-0.361889</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>2.159386</td>\n",
       "      <td>0.473854</td>\n",
       "      <td>0.541810</td>\n",
       "      <td>0.502577</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>1.237636</td>\n",
       "      <td>0.183582</td>\n",
       "      <td>0.360763</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>880</th>\n",
       "      <td>2.273267</td>\n",
       "      <td>0.514263</td>\n",
       "      <td>0.313642</td>\n",
       "      <td>0.607138</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-1.967993</td>\n",
       "      <td>-1.513224</td>\n",
       "      <td>0.752199</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>0.703903</td>\n",
       "      <td>0.776921</td>\n",
       "      <td>0.718121</td>\n",
       "      <td>0.654665</td>\n",
       "      <td>-0.378236</td>\n",
       "      <td>-0.137643</td>\n",
       "      <td>-0.777629</td>\n",
       "      <td>-0.236420</td>\n",
       "      <td>0.887696</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>4.647232e-01</td>\n",
       "      <td>-0.885456</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>1.224647e-16</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>882 rows Ã— 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "0   -0.277074 -0.101973 -0.381232 -0.134296  1.188318 -0.137643  1.603098   \n",
       "1   -0.499842 -1.172810 -1.231675 -1.351007 -0.306376 -0.137643 -2.009760   \n",
       "2   -0.332017 -1.617308 -1.345759 -1.712218 -0.378236 -0.137643  1.018358   \n",
       "3   -1.266043 -1.475877 -1.345759 -1.189412 -0.378236 -0.137643 -0.255540   \n",
       "4   -0.734597 -1.081890 -1.750238 -1.246446 -0.378236 -0.137643  0.579803   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "877  1.530042  0.726410  0.946289  0.550104 -0.335120 -0.137643  0.611129   \n",
       "878  1.462113  0.675899  0.655894  0.654665 -0.306376 -0.137643  0.861731   \n",
       "879  2.159386  0.473854  0.541810  0.502577 -0.378236 -0.137643  1.237636   \n",
       "880  2.273267  0.514263  0.313642  0.607138 -0.378236 -0.137643 -1.967993   \n",
       "881  0.703903  0.776921  0.718121  0.654665 -0.378236 -0.137643 -0.777629   \n",
       "\n",
       "           7         8       9   ...        26            27        28  \\\n",
       "0    0.133181 -1.822248  2022.0  ...  0.866025  6.432490e-16  1.000000   \n",
       "1    0.485982  0.014492  2022.0  ...  0.866025  6.432490e-16  1.000000   \n",
       "2   -0.774021  1.760901  2022.0  ...  0.866025  1.205367e-01  0.992709   \n",
       "3    0.737983  0.526370  2022.0  ...  0.866025  1.205367e-01  0.992709   \n",
       "4    3.157190 -0.994210  2022.0  ...  0.866025  1.205367e-01  0.992709   \n",
       "..        ...       ...     ...  ...       ...           ...       ...   \n",
       "877  1.275585 -1.566309  2024.0  ... -0.866025  4.647232e-01 -0.885456   \n",
       "878  0.133181 -0.361889  2024.0  ... -0.866025  4.647232e-01 -0.885456   \n",
       "879  0.183582  0.360763  2024.0  ... -0.866025  4.647232e-01 -0.885456   \n",
       "880 -1.513224  0.752199  2024.0  ... -0.866025  4.647232e-01 -0.885456   \n",
       "881 -0.236420  0.887696  2024.0  ... -0.866025  4.647232e-01 -0.885456   \n",
       "\n",
       "               29            30        31   32            33   34   35  \n",
       "0    1.000000e+00  6.123234e-17  0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "1    1.000000e+00  6.123234e-17  0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "2    1.000000e+00  6.123234e-17  0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "3    1.000000e+00  6.123234e-17  0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "4    1.000000e+00  6.123234e-17  0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "..            ...           ...       ...  ...           ...  ...  ...  \n",
       "877  1.224647e-16 -1.000000e+00 -0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "878  1.224647e-16 -1.000000e+00 -0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "879  1.224647e-16 -1.000000e+00 -0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "880  1.224647e-16 -1.000000e+00 -0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "881  1.224647e-16 -1.000000e+00 -0.866025 -0.5  1.224647e-16 -1.0  0.0  \n",
       "\n",
       "[882 rows x 36 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset_df = pd.DataFrame(full_dataset.numpy())\n",
    "full_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4ac706b7-eebf-46ed-a1d6-773eaa59bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def create_subsequences_with_duplication(full_data, seq_length, random_duplicate):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "\n",
    "    X_data = full_data[:, :-1]  \n",
    "    y_data = full_data[:, -1]  \n",
    "\n",
    "    for i in range(seq_length, len(full_data)):\n",
    "        X_seq = X_data[i - seq_length:i]\n",
    "        y_seq = y_data[i]\n",
    "        \n",
    "        sequences.append(X_seq)\n",
    "        targets.append(y_seq)\n",
    "        \n",
    "        if y_seq == 1:\n",
    "            \n",
    "            num_duplicates = 1\n",
    "            for _ in range(num_duplicates):\n",
    "                sequences.append(X_seq)\n",
    "                targets.append(y_seq)\n",
    "        elif y_seq == 2:\n",
    "            num_duplicates = 1\n",
    "            for _ in range(num_duplicates):\n",
    "                sequences.append(X_seq)\n",
    "                targets.append(y_seq)\n",
    "        elif y_seq == 3:\n",
    "            num_duplicates = 2\n",
    "            for _ in range(num_duplicates):\n",
    "                sequences.append(X_seq)\n",
    "                targets.append(y_seq)\n",
    "        elif y_seq == 4:\n",
    "            num_duplicates = 2\n",
    "            for _ in range(num_duplicates):\n",
    "                sequences.append(X_seq)\n",
    "                targets.append(y_seq)\n",
    "        \n",
    "        \n",
    "    return torch.stack(sequences), torch.tensor(targets).long()\n",
    "\n",
    "sequence_length = 5\n",
    "random_duplicate = 2\n",
    "\n",
    "X_sequences, y_sequences = create_subsequences_with_duplication(full_dataset, sequence_length, random_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "21adb1a8-b182-4d2d-8ba6-35e556d734cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([911])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c489f938-e517-4931-b31e-a73e5fa96944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "def shuffle_sequences(X_sequences, y_sequences):\n",
    "    combined = list(zip(X_sequences, y_sequences))\n",
    "    random.shuffle(combined)\n",
    "    X_shuffled, y_shuffled = zip(*combined)\n",
    "    \n",
    "    X_shuffled = torch.stack(X_shuffled)\n",
    "    y_shuffled = torch.tensor(y_shuffled).long()\n",
    "    \n",
    "    return X_shuffled, y_shuffled\n",
    "\n",
    "X_shuffled, y_shuffled = shuffle_sequences(X_sequences, y_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "dc49c7da-a77d-4920-bd1c-864c5d257d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sequences, y_sequences = X_shuffled, y_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "2d507932-a90b-4a24-83ab-300179b22686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 728\n",
      "Validation dataset size: 183\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def stratified_split(X, y, test_size=0.2):\n",
    "    unique, counts = np.unique(y, return_counts=True)\n",
    "    label_counts = dict(zip(unique, counts))\n",
    "\n",
    "    rare_indices = []\n",
    "    common_indices = []\n",
    "    for label, count in label_counts.items():\n",
    "        indices = np.where(y == label)[0]\n",
    "        if count == 1:\n",
    "            continue\n",
    "            rare_indices.extend(indices)\n",
    "        else:\n",
    "            common_indices.extend(indices)\n",
    "    \n",
    "    common_train_idx, common_val_idx = train_test_split(\n",
    "        common_indices, test_size=test_size, stratify=y[common_indices]\n",
    "    )\n",
    "    \n",
    "    train_idx = np.concatenate((common_train_idx, rare_indices)).astype(int)\n",
    "    val_idx = np.concatenate((common_val_idx, rare_indices)).astype(int)\n",
    "    \n",
    "    return train_idx, val_idx\n",
    "\n",
    "X_np = X_sequences.numpy()\n",
    "y_np = y_sequences.numpy()\n",
    "\n",
    "train_idx, val_idx = stratified_split(X_np, y_np)\n",
    "\n",
    "X_train, y_train = X_np[train_idx], y_np[train_idx]\n",
    "X_val, y_val = X_np[val_idx], y_np[val_idx]\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "train_dataset = TimeSeriesDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TimeSeriesDataset(X_val_tensor, y_val_tensor)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "83892aed-2ec7-4d6f-9b50-8d74e9d8d3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in y_val_tensor: tensor([0, 1, 2, 4])\n",
      "Counts of each label: tensor([170,  11,   1,   1])\n",
      "Unique labels in y_train_tensor: tensor([0, 1, 2, 3, 4])\n",
      "Counts of each label: tensor([675,  43,   5,   3,   2])\n"
     ]
    }
   ],
   "source": [
    "unique_labels, counts = y_val_tensor.unique(return_counts=True)\n",
    "print(f\"Unique labels in y_val_tensor: {unique_labels}\")\n",
    "print(f\"Counts of each label: {counts}\")\n",
    "\n",
    "unique_labels, counts = y_train_tensor.unique(return_counts=True)\n",
    "print(f\"Unique labels in y_train_tensor: {unique_labels}\")\n",
    "print(f\"Counts of each label: {counts}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "293be5cc-dd5a-4041-aef3-f024fadd2bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique y values in validation dataset:\n",
      "Class 0: 170 samples\n",
      "Class 1: 11 samples\n",
      "Class 2: 1 samples\n",
      "Class 4: 1 samples\n",
      "\n",
      "Class distribution in validation dataset:\n",
      "Class 0: 92.90%\n",
      "Class 1: 6.01%\n",
      "Class 2: 0.55%\n",
      "Class 4: 0.55%\n",
      "\n",
      "Detailed count of y values:\n",
      "Counter({tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(4): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(2): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "val_y_values = [y for _, y in val_dataset]\n",
    "\n",
    "val_y_tensor = torch.tensor(val_y_values)\n",
    "\n",
    "unique_values, counts = torch.unique(val_y_tensor, return_counts=True)\n",
    "\n",
    "print(\"Unique y values in validation dataset:\")\n",
    "for value, count in zip(unique_values.tolist(), counts.tolist()):\n",
    "    print(f\"Class {value}: {count} samples\")\n",
    "\n",
    "total_samples = len(val_y_values)\n",
    "print(\"\\nClass distribution in validation dataset:\")\n",
    "for value, count in zip(unique_values.tolist(), counts.tolist()):\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"Class {value}: {percentage:.2f}%\")\n",
    "\n",
    "val_y_counter = Counter(val_y_values)\n",
    "print(\"\\nDetailed count of y values:\")\n",
    "print(val_y_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cc0ee186-d1b1-404c-99cf-9bfac5fdbeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique y values in train dataset:\n",
      "Class 0: 675 samples\n",
      "Class 1: 43 samples\n",
      "Class 2: 5 samples\n",
      "Class 3: 3 samples\n",
      "Class 4: 2 samples\n",
      "\n",
      "Class distribution in train dataset:\n",
      "Class 0: 92.72%\n",
      "Class 1: 5.91%\n",
      "Class 2: 0.69%\n",
      "Class 3: 0.41%\n",
      "Class 4: 0.27%\n",
      "\n",
      "Detailed count of y values:\n",
      "Counter({tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(4): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(2): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(1): 1, tensor(1): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1, tensor(0): 1})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "train_y_values = [y for _, y in train_dataset]\n",
    "\n",
    "train_y_tensor = torch.tensor(train_y_values)\n",
    "\n",
    "unique_values, counts = torch.unique(train_y_tensor, return_counts=True)\n",
    "\n",
    "print(\"Unique y values in train dataset:\")\n",
    "for value, count in zip(unique_values.tolist(), counts.tolist()):\n",
    "    print(f\"Class {value}: {count} samples\")\n",
    "\n",
    "total_samples = len(train_y_values)\n",
    "print(\"\\nClass distribution in train dataset:\")\n",
    "for value, count in zip(unique_values.tolist(), counts.tolist()):\n",
    "    percentage = (count / total_samples) * 100\n",
    "    print(f\"Class {value}: {percentage:.2f}%\")\n",
    "\n",
    "train_y_counter = Counter(val_y_values)\n",
    "print(\"\\nDetailed count of y values:\")\n",
    "print(train_y_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d325fd7e-7f4f-432d-9f48-37143e584862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batch shapes: X: torch.Size([16, 5, 35]), y: torch.Size([16])\n",
      "Validation batch shapes: X: torch.Size([16, 5, 35]), y: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "for X_batch, y_batch in train_loader:\n",
    "    print(f\"Train batch shapes: X: {X_batch.shape}, y: {y_batch.shape}\")\n",
    "    break\n",
    "\n",
    "for X_batch, y_batch in val_loader:\n",
    "    print(f\"Validation batch shapes: X: {X_batch.shape}, y: {y_batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "1e7268f9-84f8-4456-bea3-09318706e897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GatedLinearUnit(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(GatedLinearUnit, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.gate = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) * torch.sigmoid(self.gate(x))\n",
    "\n",
    "\n",
    "class LayerNormLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, bidirectional):\n",
    "        super(LayerNormLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers=num_layers,\n",
    "            batch_first=True, dropout=dropout, bidirectional=bidirectional\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * (2 if bidirectional else 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        return lstm_out\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.long, device=x.device).unsqueeze(0)\n",
    "        pos_enc = self.pos_embedding(positions)\n",
    "        return x + pos_enc\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, lstm_dropout=0.3, fcn_dropout=0.5):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.bidirectional = True\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "        \n",
    "        self.num_heads = 16\n",
    "        self.hidden_size = (hidden_size // (self.num_directions * 3 * self.num_heads)) * self.num_heads\n",
    "        \n",
    "        print(f\"Adjusted hidden_size: {self.hidden_size}\")\n",
    "        \n",
    "        self.embedding_size = input_size\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(self.embedding_size)\n",
    "\n",
    "        self.lstm1 = LayerNormLSTM(\n",
    "            self.embedding_size, self.hidden_size, num_layers=num_layers,\n",
    "            dropout=lstm_dropout, bidirectional=self.bidirectional\n",
    "        )\n",
    "        self.lstm2 = ResidualLSTM(\n",
    "            self.embedding_size, self.hidden_size, num_layers=num_layers,\n",
    "            dropout=lstm_dropout, bidirectional=self.bidirectional\n",
    "        )\n",
    "        self.lstm3 = LayerDropLSTM(\n",
    "            self.embedding_size, self.hidden_size, num_layers=num_layers,\n",
    "            dropout=lstm_dropout, bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "        self.combined_lstm_size = self.hidden_size * self.num_directions * 3\n",
    "        \n",
    "        print(f\"Combined LSTM size: {self.combined_lstm_size}\")\n",
    "        print(f\"Number of heads: {self.num_heads}\")\n",
    "        print(f\"Is combined_lstm_size divisible by num_heads? {self.combined_lstm_size % self.num_heads == 0}\")\n",
    "\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.combined_lstm_size,\n",
    "            nhead=self.num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.transformer_layer, num_layers=1\n",
    "        )\n",
    "\n",
    "        self.attention = MultiHeadSelfAttention(self.combined_lstm_size, self.num_heads)\n",
    "\n",
    "        self.glu1 = GatedLinearUnit(self.combined_lstm_size, self.combined_lstm_size // 2)\n",
    "        self.glu2 = GatedLinearUnit(self.combined_lstm_size // 2, self.combined_lstm_size // 4)\n",
    "\n",
    "        self.fc = nn.Linear(self.combined_lstm_size // 4, num_classes)\n",
    "\n",
    "        self.layer_norm2 = nn.LayerNorm(self.combined_lstm_size // 2)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.combined_lstm_size // 4)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=fcn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        lstm_out1 = self.lstm1(x)\n",
    "        lstm_out2 = self.lstm2(x)\n",
    "        lstm_out3 = self.lstm3(x)\n",
    "\n",
    "        lstm_out_concat = torch.cat((lstm_out1, lstm_out2, lstm_out3), dim=-1)\n",
    "\n",
    "        transformer_out = self.transformer_encoder(lstm_out_concat)\n",
    "        transformer_out = lstm_out_concat + transformer_out  \n",
    "\n",
    "        attn_out = self.attention(transformer_out)\n",
    "        attn_out = transformer_out + attn_out  \n",
    "\n",
    "\n",
    "        global_avg_pool = torch.mean(attn_out, dim=1)\n",
    "\n",
    "        out = self.glu1(global_avg_pool)\n",
    "        out = self.layer_norm2(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.glu2(out)\n",
    "        out = self.layer_norm3(out)\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean', label_smoothing=0.1):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        num_classes = inputs.size(1)\n",
    "        smoothed_labels = F.one_hot(targets, num_classes=num_classes)\n",
    "        smoothed_labels = smoothed_labels * (1 - self.label_smoothing) + self.label_smoothing / num_classes\n",
    "\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=self.label_smoothing)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "b11aa87f-f0d0-4664-bcfc-57a8626b9b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GatedLinearUnit(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(GatedLinearUnit, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.gate = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) * torch.sigmoid(self.gate(x))\n",
    "\n",
    "class LayerNormLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, bidirectional):\n",
    "        super(LayerNormLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers=num_layers,\n",
    "            batch_first=True, dropout=dropout, bidirectional=bidirectional\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * (2 if bidirectional else 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        return lstm_out\n",
    "\n",
    "class LayerDropLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, bidirectional, layer_drop_prob=0.2):\n",
    "        super(LayerDropLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.layer_drop_prob = layer_drop_prob\n",
    "        self.projection = nn.Linear(input_size, hidden_size * (2 if bidirectional else 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training and torch.rand(1).item() < self.layer_drop_prob:\n",
    "            return self.projection(x)  \n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        return lstm_out\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=hidden_size, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        return attn_output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.long, device=x.device).unsqueeze(0)\n",
    "        pos_enc = self.pos_embedding(positions)\n",
    "        return x + pos_enc\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "\n",
    "class ResidualLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout, bidirectional):\n",
    "        super(ResidualLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers,\n",
    "                            batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
    "        self.projection = nn.Linear(hidden_size * (2 if bidirectional else 1), hidden_size * (2 if bidirectional else 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        projected_out = self.projection(lstm_out)\n",
    "        return projected_out\n",
    "\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, lstm_dropout=0.3, fcn_dropout=0.5, debug=False):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.bidirectional = True\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "        self.debug = debug\n",
    "        \n",
    "        self.num_heads = 16\n",
    "        self.hidden_size = hidden_size // (self.num_directions * 3)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Adjusted hidden_size: {self.hidden_size}\")\n",
    "        \n",
    "        self.embedding_size = input_size\n",
    "\n",
    "        self.positional_encoding = PositionalEncoding(self.embedding_size)\n",
    "\n",
    "        self.lstm1 = LayerNormLSTM(\n",
    "            self.embedding_size, self.hidden_size, num_layers=num_layers,\n",
    "            dropout=lstm_dropout, bidirectional=self.bidirectional\n",
    "        )\n",
    "        self.lstm2 = ResidualLSTM(\n",
    "            self.embedding_size, self.hidden_size, num_layers=num_layers,\n",
    "            dropout=lstm_dropout, bidirectional=self.bidirectional\n",
    "        )\n",
    "        self.lstm3 = LayerDropLSTM(\n",
    "            self.embedding_size, self.hidden_size, num_layers=num_layers,\n",
    "            dropout=lstm_dropout, bidirectional=self.bidirectional\n",
    "        )\n",
    "\n",
    "        self.combined_lstm_size = self.hidden_size * self.num_directions * 3\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Combined LSTM size: {self.combined_lstm_size}\")\n",
    "            print(f\"Number of heads: {self.num_heads}\")\n",
    "            print(f\"Is combined_lstm_size divisible by num_heads? {self.combined_lstm_size % self.num_heads == 0}\")\n",
    "\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=self.combined_lstm_size,\n",
    "            nhead=self.num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.transformer_layer, num_layers=3\n",
    "        )\n",
    "\n",
    "        self.attention = MultiHeadSelfAttention(self.combined_lstm_size, self.num_heads)\n",
    "\n",
    "        self.glu1 = GatedLinearUnit(self.combined_lstm_size, self.combined_lstm_size // 2)\n",
    "        self.glu2 = GatedLinearUnit(self.combined_lstm_size // 2, self.combined_lstm_size // 4)\n",
    "\n",
    "        self.fc = nn.Linear(self.combined_lstm_size // 4, num_classes)\n",
    "\n",
    "        self.layer_norm2 = nn.LayerNorm(self.combined_lstm_size // 2)\n",
    "        self.layer_norm3 = nn.LayerNorm(self.combined_lstm_size // 4)\n",
    "\n",
    "        self.dropout = nn.Dropout(p=fcn_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after positional encoding: {x.shape}\")\n",
    "\n",
    "\n",
    "        lstm_out1 = self.lstm1(x)\n",
    "        lstm_out2 = self.lstm2(x)\n",
    "        lstm_out3 = self.lstm3(x)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape of lstm_out1: {lstm_out1.shape}\")\n",
    "            print(f\"Shape of lstm_out2: {lstm_out2.shape}\")\n",
    "            print(f\"Shape of lstm_out3: {lstm_out3.shape}\")\n",
    "\n",
    "        lstm_out_concat = torch.cat((lstm_out1, lstm_out2, lstm_out3), dim=-1)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after concatenation: {lstm_out_concat.shape}\")\n",
    "        transformer_out = self.transformer_encoder(lstm_out_concat)\n",
    "        transformer_out = lstm_out_concat + transformer_out  #Adding residual connection\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after transformer: {transformer_out.shape}\")\n",
    "\n",
    "        #Attention mechanism\n",
    "        attn_out = self.attention(transformer_out)\n",
    "        attn_out = transformer_out + attn_out  #Adding residual connection\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after attention: {attn_out.shape}\")\n",
    "\n",
    "        #Global average pooling\n",
    "        global_avg_pool = torch.mean(attn_out, dim=1)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after global average pooling: {global_avg_pool.shape}\")\n",
    "\n",
    "        #Passing through GLUs\n",
    "        out = self.glu1(global_avg_pool)\n",
    "        out = self.layer_norm2(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after first GLU: {out.shape}\")\n",
    "\n",
    "        out = self.glu2(out)\n",
    "        out = self.layer_norm3(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Shape after second GLU: {out.shape}\")\n",
    "\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        if self.debug:\n",
    "            print(f\"Final output shape: {out.shape}\")\n",
    "\n",
    "        return out\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean', label_smoothing=0.1):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.label_smoothing = label_smoothing\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        num_classes = inputs.size(1)\n",
    "        smoothed_labels = F.one_hot(targets, num_classes=num_classes)\n",
    "        smoothed_labels = smoothed_labels * (1 - self.label_smoothing) + self.label_smoothing / num_classes\n",
    "\n",
    "        ce_loss = F.cross_entropy(inputs, targets, reduction='none', label_smoothing=self.label_smoothing)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "c1e816ac-b303-41e6-95ea-33721536a967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([911, 5, 35])"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "54fd93b8-82c6-4801-9bd5-cf172c2a999a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size: 35\n",
      "Initial hidden size: 384\n",
      "Adjusted hidden_size: 64\n",
      "Combined LSTM size: 384\n",
      "Number of heads: 16\n",
      "Is combined_lstm_size divisible by num_heads? True\n",
      "Shape after positional encoding: torch.Size([16, 100, 35])\n",
      "Shape of lstm_out1: torch.Size([16, 100, 128])\n",
      "Shape of lstm_out2: torch.Size([16, 100, 128])\n",
      "Shape of lstm_out3: torch.Size([16, 100, 128])\n",
      "Shape after concatenation: torch.Size([16, 100, 384])\n",
      "Shape after transformer: torch.Size([16, 100, 384])\n",
      "Shape after attention: torch.Size([16, 100, 384])\n",
      "Shape after global average pooling: torch.Size([16, 384])\n",
      "Shape after first GLU: torch.Size([16, 192])\n",
      "Shape after second GLU: torch.Size([16, 96])\n",
      "Final output shape: torch.Size([16, 5])\n"
     ]
    }
   ],
   "source": [
    "input_size = X_sequences.shape[2] #should match your X_sequences.shape[2] which is 35\n",
    "hidden_size = 384  #should be divisible by (num_directions * 3 * num_heads)\n",
    "num_layers = 4\n",
    "num_classes = 5\n",
    "\n",
    "print(f\"Input size: {input_size}\")\n",
    "print(f\"Initial hidden size: {hidden_size}\")\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes, debug=True)\n",
    "\n",
    "batch_size = 16\n",
    "sequence_length = 100\n",
    "dummy_input = torch.randn(batch_size, sequence_length, input_size)\n",
    "\n",
    "output = model(dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "3d53ca8f-3159-4171-8fc2-9ea8f7e4b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X_sequences.shape[2]\n",
    "hidden_size = 384 #should be divisible by (num_directions * 3 * num_heads)\n",
    "num_layers = 4\n",
    "num_layers = 4\n",
    "num_epochs = 70\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "3f64a17d-03b0-48de-ae4f-3c60efaf42a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (pos_embedding): Embedding(5000, 35)\n",
       "  )\n",
       "  (lstm1): LayerNormLSTM(\n",
       "    (lstm): LSTM(35, 64, num_layers=4, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "    (layer_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lstm2): ResidualLSTM(\n",
       "    (lstm): LSTM(35, 64, num_layers=4, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "    (projection): Linear(in_features=128, out_features=128, bias=True)\n",
       "  )\n",
       "  (lstm3): LayerDropLSTM(\n",
       "    (lstm): LSTM(35, 64, num_layers=4, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "    (projection): Linear(in_features=35, out_features=128, bias=True)\n",
       "  )\n",
       "  (transformer_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
       "    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=384, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=384, bias=True)\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (attention): MultiHeadSelfAttention(\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (glu1): GatedLinearUnit(\n",
       "    (linear): Linear(in_features=384, out_features=192, bias=True)\n",
       "    (gate): Linear(in_features=384, out_features=192, bias=True)\n",
       "  )\n",
       "  (glu2): GatedLinearUnit(\n",
       "    (linear): Linear(in_features=192, out_features=96, bias=True)\n",
       "    (gate): Linear(in_features=192, out_features=96, bias=True)\n",
       "  )\n",
       "  (fc): Linear(in_features=96, out_features=5, bias=True)\n",
       "  (layer_norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "  (layer_norm3): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LSTMModel(input_size, hidden_size, num_layers, num_classes, debug = False)\n",
    "criterion = FocalLoss(alpha=1, gamma=2, label_smoothing=0.1)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-7)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "9a988e17-65a5-4878-80e1-254563eebd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights initialized.\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.LSTM):\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                nn.init.xavier_uniform_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                nn.init.orthogonal_(param.data)\n",
    "            elif 'bias_ih' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "                #setting forget gate bias to 1\n",
    "                n = param.size(0)\n",
    "                start, end = n // 4, n // 2\n",
    "                param.data[start:end].fill_(1.)\n",
    "            elif 'bias_hh' in name:\n",
    "                nn.init.zeros_(param.data)\n",
    "    elif isinstance(m, nn.LayerNorm):\n",
    "        nn.init.ones_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "    elif isinstance(m, nn.MultiheadAttention):\n",
    "        if m.in_proj_weight is not None:\n",
    "            nn.init.xavier_uniform_(m.in_proj_weight)\n",
    "        if m.out_proj.weight is not None:\n",
    "            nn.init.xavier_uniform_(m.out_proj.weight)\n",
    "        if m.in_proj_bias is not None:\n",
    "            nn.init.zeros_(m.in_proj_bias)\n",
    "        if m.out_proj.bias is not None:\n",
    "            nn.init.zeros_(m.out_proj.bias)\n",
    "\n",
    "model.apply(init_weights)\n",
    "\n",
    "print(\"Model weights initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "82cec539-d5dc-4d92-a5a0-315013d2a434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)\n",
    "\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=9)\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=0.001, steps_per_epoch=len(train_loader), epochs=num_epochs\n",
    ")\n",
    "\n",
    "# scheduler = optim.lr_scheduler.CyclicLR(\n",
    "#     optimizer, base_lr=0.0001, max_lr=0.01, step_size_up=5, mode='triangular2'\n",
    "# )\n",
    "\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "ba0cdaa4-b044-4c07-af6e-f7bfc3d9218a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/70], Train Loss: 0.3912, Train Acc: 81.32%, Val Loss: 0.2539, Val Acc: 92.90%\n",
      "Epoch [2/70], Train Loss: 0.2750, Train Acc: 92.72%, Val Loss: 0.2466, Val Acc: 92.90%\n",
      "Epoch [3/70], Train Loss: 0.2408, Train Acc: 92.58%, Val Loss: 0.2178, Val Acc: 92.90%\n",
      "Epoch [4/70], Train Loss: 0.2344, Train Acc: 92.72%, Val Loss: 0.2228, Val Acc: 92.90%\n",
      "Epoch [5/70], Train Loss: 0.2542, Train Acc: 92.58%, Val Loss: 0.2279, Val Acc: 92.90%\n",
      "Epoch [6/70], Train Loss: 0.2379, Train Acc: 92.58%, Val Loss: 0.2182, Val Acc: 92.90%\n",
      "Epoch [7/70], Train Loss: 0.2391, Train Acc: 92.72%, Val Loss: 0.2118, Val Acc: 92.90%\n",
      "Epoch [8/70], Train Loss: 0.2458, Train Acc: 92.72%, Val Loss: 0.2133, Val Acc: 92.90%\n",
      "Epoch [9/70], Train Loss: 0.2372, Train Acc: 92.72%, Val Loss: 0.2313, Val Acc: 92.90%\n",
      "Epoch [10/70], Train Loss: 0.2391, Train Acc: 92.72%, Val Loss: 0.2315, Val Acc: 92.90%\n",
      "Epoch [11/70], Train Loss: 0.2336, Train Acc: 92.45%, Val Loss: 0.2115, Val Acc: 92.90%\n",
      "Epoch [12/70], Train Loss: 0.2192, Train Acc: 92.72%, Val Loss: 0.2054, Val Acc: 92.90%\n",
      "Epoch [13/70], Train Loss: 0.2198, Train Acc: 92.72%, Val Loss: 0.1966, Val Acc: 92.90%\n",
      "Epoch [14/70], Train Loss: 0.2341, Train Acc: 92.72%, Val Loss: 0.2033, Val Acc: 92.90%\n",
      "Epoch [15/70], Train Loss: 0.2250, Train Acc: 92.72%, Val Loss: 0.2045, Val Acc: 92.90%\n",
      "Epoch [16/70], Train Loss: 0.2240, Train Acc: 92.72%, Val Loss: 0.2133, Val Acc: 92.90%\n",
      "Epoch [17/70], Train Loss: 0.2240, Train Acc: 92.72%, Val Loss: 0.2010, Val Acc: 92.90%\n",
      "Epoch [18/70], Train Loss: 0.2237, Train Acc: 92.72%, Val Loss: 0.1940, Val Acc: 92.90%\n",
      "Epoch [19/70], Train Loss: 0.2203, Train Acc: 92.72%, Val Loss: 0.1851, Val Acc: 92.90%\n",
      "Epoch [20/70], Train Loss: 0.2162, Train Acc: 92.45%, Val Loss: 0.1895, Val Acc: 92.90%\n",
      "Epoch [21/70], Train Loss: 0.2197, Train Acc: 92.72%, Val Loss: 0.1978, Val Acc: 92.90%\n",
      "Epoch [22/70], Train Loss: 0.2262, Train Acc: 92.72%, Val Loss: 0.2016, Val Acc: 92.90%\n",
      "Epoch [23/70], Train Loss: 0.2124, Train Acc: 92.72%, Val Loss: 0.1942, Val Acc: 92.90%\n",
      "Epoch [24/70], Train Loss: 0.2182, Train Acc: 92.58%, Val Loss: 0.1837, Val Acc: 92.90%\n",
      "Epoch [25/70], Train Loss: 0.2138, Train Acc: 92.72%, Val Loss: 0.1818, Val Acc: 92.90%\n",
      "Epoch [26/70], Train Loss: 0.2160, Train Acc: 92.31%, Val Loss: 0.1825, Val Acc: 92.90%\n",
      "Epoch [27/70], Train Loss: 0.2112, Train Acc: 92.86%, Val Loss: 0.1806, Val Acc: 92.90%\n",
      "Epoch [28/70], Train Loss: 0.2159, Train Acc: 92.58%, Val Loss: 0.1864, Val Acc: 92.90%\n",
      "Epoch [29/70], Train Loss: 0.2100, Train Acc: 92.72%, Val Loss: 0.1819, Val Acc: 92.90%\n",
      "Epoch [30/70], Train Loss: 0.1942, Train Acc: 92.72%, Val Loss: 0.1758, Val Acc: 92.90%\n",
      "Epoch [31/70], Train Loss: 0.2174, Train Acc: 92.72%, Val Loss: 0.1774, Val Acc: 92.90%\n",
      "Epoch [32/70], Train Loss: 0.2109, Train Acc: 92.86%, Val Loss: 0.1725, Val Acc: 92.90%\n",
      "Epoch [33/70], Train Loss: 0.2195, Train Acc: 92.58%, Val Loss: 0.1948, Val Acc: 92.90%\n",
      "Epoch [34/70], Train Loss: 0.2206, Train Acc: 92.72%, Val Loss: 0.1948, Val Acc: 92.90%\n",
      "Epoch [35/70], Train Loss: 0.2128, Train Acc: 92.72%, Val Loss: 0.1853, Val Acc: 92.90%\n",
      "Epoch [36/70], Train Loss: 0.2261, Train Acc: 92.86%, Val Loss: 0.1718, Val Acc: 92.90%\n",
      "Epoch [37/70], Train Loss: 0.1972, Train Acc: 92.45%, Val Loss: 0.1762, Val Acc: 92.90%\n",
      "Epoch [38/70], Train Loss: 0.2160, Train Acc: 92.86%, Val Loss: 0.1687, Val Acc: 92.90%\n",
      "Epoch [39/70], Train Loss: 0.2205, Train Acc: 92.58%, Val Loss: 0.1739, Val Acc: 92.90%\n",
      "Epoch [40/70], Train Loss: 0.2091, Train Acc: 92.58%, Val Loss: 0.1664, Val Acc: 92.90%\n",
      "Epoch [41/70], Train Loss: 0.1992, Train Acc: 92.72%, Val Loss: 0.1663, Val Acc: 92.90%\n",
      "Epoch [42/70], Train Loss: 0.2183, Train Acc: 92.72%, Val Loss: 0.1797, Val Acc: 92.90%\n",
      "Epoch [43/70], Train Loss: 0.2096, Train Acc: 92.72%, Val Loss: 0.1677, Val Acc: 92.90%\n",
      "Epoch [44/70], Train Loss: 0.2104, Train Acc: 92.72%, Val Loss: 0.1756, Val Acc: 92.90%\n",
      "Epoch [45/70], Train Loss: 0.2132, Train Acc: 92.72%, Val Loss: 0.1831, Val Acc: 92.90%\n",
      "Epoch [46/70], Train Loss: 0.2105, Train Acc: 92.45%, Val Loss: 0.1704, Val Acc: 92.90%\n",
      "Epoch [47/70], Train Loss: 0.2055, Train Acc: 92.72%, Val Loss: 0.1635, Val Acc: 92.90%\n",
      "Epoch [48/70], Train Loss: 0.2215, Train Acc: 92.45%, Val Loss: 0.2012, Val Acc: 92.90%\n",
      "Epoch [49/70], Train Loss: 0.2120, Train Acc: 92.72%, Val Loss: 0.1840, Val Acc: 92.90%\n",
      "Epoch [50/70], Train Loss: 0.2059, Train Acc: 92.86%, Val Loss: 0.1650, Val Acc: 92.90%\n",
      "Epoch [51/70], Train Loss: 0.2037, Train Acc: 92.45%, Val Loss: 0.1681, Val Acc: 92.90%\n",
      "Epoch [52/70], Train Loss: 0.2065, Train Acc: 92.58%, Val Loss: 0.1815, Val Acc: 92.90%\n",
      "Epoch [53/70], Train Loss: 0.2145, Train Acc: 92.72%, Val Loss: 0.1834, Val Acc: 92.90%\n",
      "Epoch [54/70], Train Loss: 0.2114, Train Acc: 92.72%, Val Loss: 0.2025, Val Acc: 92.90%\n",
      "Epoch [55/70], Train Loss: 0.2177, Train Acc: 92.72%, Val Loss: 0.2028, Val Acc: 92.90%\n",
      "Epoch [56/70], Train Loss: 0.2183, Train Acc: 92.72%, Val Loss: 0.1770, Val Acc: 92.90%\n",
      "Epoch [57/70], Train Loss: 0.2059, Train Acc: 92.45%, Val Loss: 0.1673, Val Acc: 92.90%\n",
      "Epoch [58/70], Train Loss: 0.2110, Train Acc: 92.72%, Val Loss: 0.1866, Val Acc: 92.90%\n",
      "Epoch [59/70], Train Loss: 0.2104, Train Acc: 92.72%, Val Loss: 0.1705, Val Acc: 92.90%\n",
      "Epoch [60/70], Train Loss: 0.2037, Train Acc: 92.58%, Val Loss: 0.1623, Val Acc: 92.90%\n",
      "Epoch [61/70], Train Loss: 0.2091, Train Acc: 92.58%, Val Loss: 0.1895, Val Acc: 92.90%\n",
      "Epoch [62/70], Train Loss: 0.2221, Train Acc: 92.72%, Val Loss: 0.1792, Val Acc: 92.90%\n",
      "Epoch [63/70], Train Loss: 0.2049, Train Acc: 92.58%, Val Loss: 0.1660, Val Acc: 92.90%\n",
      "Epoch [64/70], Train Loss: 0.2064, Train Acc: 92.72%, Val Loss: 0.2038, Val Acc: 92.90%\n",
      "Epoch [65/70], Train Loss: 0.2212, Train Acc: 92.72%, Val Loss: 0.1880, Val Acc: 92.90%\n",
      "Epoch [66/70], Train Loss: 0.2075, Train Acc: 92.58%, Val Loss: 0.1637, Val Acc: 92.90%\n",
      "Epoch [67/70], Train Loss: 0.2123, Train Acc: 92.72%, Val Loss: 0.1689, Val Acc: 92.90%\n",
      "Epoch [68/70], Train Loss: 0.2094, Train Acc: 92.72%, Val Loss: 0.1617, Val Acc: 92.90%\n",
      "Epoch [69/70], Train Loss: 0.2175, Train Acc: 92.72%, Val Loss: 0.1841, Val Acc: 92.90%\n",
      "Epoch [70/70], Train Loss: 0.2194, Train Acc: 92.72%, Val Loss: 0.1837, Val Acc: 92.90%\n"
     ]
    }
   ],
   "source": [
    "early_stop_patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = 100 * correct / total\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}%, '\n",
    "          f'Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.2f}%')\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        best_model = model.state_dict()\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stop_patience:\n",
    "            print(\"Early stopping!\")\n",
    "            model.load_state_dict(best_model)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "b8b3b79a-b316-4a4f-98d2-95ed1768d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'best_lstm_model_improved.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "69abf19a-bbd6-4e75-affe-7907e1848406",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_2024_test_cleaned.to_csv('X_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "5cad4b5e-5e9c-4557-b0b8-c609bee41baa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133, 12)"
      ]
     },
     "execution_count": 466,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "fc3d2a2b-df9e-4422-b17a-e5dfc80e2bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_numeric = merged_data_2024_test_cleaned.select_dtypes(include=[float, int])\n",
    "\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_numeric.values, dtype=torch.float32)\n",
    "\n",
    "def create_sequences_test(X, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(seq_length, len(X)):\n",
    "        X_seq = X[i-seq_length:i]\n",
    "        sequences.append(X_seq)\n",
    "    return torch.stack(sequences)\n",
    "\n",
    "sequence_length = 5\n",
    "X_test_sequences = create_sequences_test(X_test_tensor, sequence_length)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_sequences)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "62a1b9e4-d324-45f1-bf7d-bca773321ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_lstm_model_improved.pth', weights_only=True))\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "sequence_predictions = []\n",
    "with torch.no_grad():\n",
    "    for X_batch in test_loader:\n",
    "        X_batch = X_batch[0].to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        sequence_predictions.extend(predicted.cpu().numpy())\n",
    "        \n",
    "individual_predictions = []\n",
    "for i in range(len(X_test_numeric) - sequence_length):\n",
    "    individual_predictions.append(sequence_predictions[i])\n",
    "\n",
    "for i in range(sequence_length):\n",
    "    individual_predictions.insert(0, individual_predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "915e24b5-2209-437d-be58-fdc2d0465558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID Predicted_AQI\n",
      "0   1          Good\n",
      "1   2          Good\n",
      "2   3          Good\n",
      "3   4          Good\n",
      "4   5          Good\n",
      "Total predictions: 133\n",
      "Original data points: 133\n"
     ]
    }
   ],
   "source": [
    "label_mapping_legend = {'Good': 0, 'Moderate': 1, 'Poor': 2, 'Severe': 3, 'Unhealthy': 4}\n",
    "reverse_label_mapping = {v: k for k, v in label_mapping_legend.items()}\n",
    "test_predictions_labels = pd.Series([reverse_label_mapping[pred] for pred in individual_predictions], name='Predicted_AQI')\n",
    "ID_column = pd.Series(range(1, len(individual_predictions) + 1), name='ID')\n",
    "predictions_df = pd.concat([ID_column, test_predictions_labels], axis=1)\n",
    "\n",
    "predictions_df.to_csv('test_predictions_with_labels.csv', index=False)\n",
    "print(predictions_df.head())\n",
    "print(f\"Total predictions: {len(predictions_df)}\")\n",
    "print(f\"Original data points: {len(X_test_numeric)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "01c5ff60-f4ac-4795-b0a7-ead79b4b1fa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Predicted_AQI\n",
       "Good    133\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_value_counts = predictions_df['Predicted_AQI'].value_counts()\n",
    "unique_value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d84a9a0-19c3-4db6-bba3-39accda03869",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
